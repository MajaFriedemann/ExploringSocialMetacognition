---
title: "Exploring Social Metacognition - Grid Repetition comparison"
author: Matt Jaquiery (matt.jaquiery@psy.ox.ac.uk)
output: 
  html_document:
    toc: false
    toc_depth: 3
    includes:
      after_body: src/toc_menu.html
editor_options: 
  chunk_output_type: console
---
December 2018  
[Script run `r Sys.time()`]

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
startTime <- Sys.time()
source('src/ESM_core.R')
```

# Descriptives 

## Introduction 

Participants performed a perceptual discrimination task in which they had to identify which of two boxes contained the most dots. Stimuli were presented for a brief time.

<div style="display: flex; justify-content: space-between">
  <div style="text-align: center">![Task screenshot](img/dot_task.png)<br/>
      The Dot Task
  </div>
</div>

## Load data  

```{r}
# folderName <- "G:\\Documents\\University\\Google Drive\\Temp\\data\\processed"

# results <- loadFilesFromFolder(folderName)
# results <- removeParticipantIds(results)

# folderName <- 'ESM_sim.R'
# source('src/ESM_sim.R')
# results <- simulateAdvisorChoice(16, aPairs = list('agr' = c(7,8)), tTypes = c(trialTypes$dual, trialTypes$change))

# unpack results
for(i in 1:length(results))
  assign(names(results)[i], results[i][[1]])

cat(paste('Loaded data from', folderName))
```

```{r}
trials <- cbind(trials, trialUtilityVariables(results))
all.trials <- trials
trials <- trials[trials$practice == F, ]
cat('Generated utility variables')
```

## Metadata

Responses (`r paste('*N* =', length(unique(participants$pid)))`) were collected between `r as.POSIXct(min(unlist(participants$timeStart))/1000, tz = '', origin = '1970-01-01')` and `r as.POSIXct(max(unlist(participants$timeEnd))/1000, tz = '', origin = '1970-01-01')`.

Demographic data are not collected and therefore not analysed. Participants must be over 18 years old to use the Prolific recruitment platform.

# Exclusions

```{r}
participants$excluded <- sapply(participants$pid, function(pid){
  ts <- all.trials[all.trials$pid == pid,]
  # overall accuracy of initial decisions
  m <- mean(ts$initialCorrect, na.rm = T)
  if(m < .6 | m > .85) return('Accuracy')
  # varied use of confidence scale
  cCs <- aggregate(pid ~ confidenceCategory, data = ts, FUN = length)
  # All confidence categories must be used
  if(nrow(cCs) < 3) return ('Confident')
  # All confidence categories must have at least 5% of the number of trials
  if(any(cCs$pid < length(ts)*.05)) return('<5%')
  return(F)
  })
# exclude on the basis of collecting too much data
if(sum(participants$excluded == F) > 50) {
  tmp <- participants[participants$excluded == F, c('id', 'timeStart')]
  tmp <- tmp$id[order(tmp$timeStart)]
  tmp <- tmp[1:50]
  participants$excluded[!(participants$id %in% tmp)] <- 'Excess'
}
```

We exclude participants for:

* Proportion of correct initial judgements must be (.60 < cor1/n < .90) (*N* = `r sum(participants$excluded == 'Accuracy')`)

* Having fewer than 3 confidence categories (*N* = `r sum(participants$excluded == 'Confidence')`)

* Having fewer than 5% of trials in each confidence category (*N* = `r sum(participants$excluded == '<5%')`)

* There being more data collected than specified in pre-registration (*N* = `r sum(participants$excluded == 'Excess')`)

*NB: **practice trials are included** in this since they are used in part for determining confidence calibration*

The number of participants analysed after exclusions (total *N* = `r sum(participants$excluded != F)`) have taken place is `r sum(participants$excluded == F)`.

```{r}
# Perform exclusions
participants <- participants[participants$excluded==F, ]
# Remove excluded participants' data from other data frames
all.trials <- all.trials[all.trials$pid %in% participants$pid, ]
trials <- trials[trials$pid %in% participants$pid, ]
advisors <- advisors[advisors$pid %in% participants$pid, ]
questionnaires <- questionnaires[questionnaires$pid %in% participants$pid, ]
genTrustQ <- genTrustQ[genTrustQ$pid %in% participants$pid, ]
```

# Comparisons

## Type 1 performance

Participants' performance was held at 71% by design. Participants' mean percentage correct  was `r md.mean(aggregate(initialCorrect ~ pid, trials, mean)$initialCorrect, isProportion = T)`. 

Type 1 performance should not differ as a function of repetition. This is checked by comparing the performance on the first appearance of repeated trials against performance on the repetitions.

```{r results = 'asis'}
tmp <- trials[trials$grid %in% trials$grid[!is.na(trials$stimulusParent)], ]
m <- is.na(tmp$stimulusParent)
novel <- aggregate(initialCorrect ~ pid, data = tmp[m, ], FUN = mean)
repeated <- aggregate(initialCorrect ~ pid, data = tmp[!m, ], FUN = mean)
md.ttestBF(novel, repeated, labels = c('*M*|novel', '*M*|repeat'), paired = T)
```

If there are differences, these may be due to:

* (implicit) memory effects  

* a tendency for repeated trials to be easier due to the trimming process


Some trials have their grids switch side when repeated. Insofar as there is a difference in the original and repeated presentations, this difference should be attenuated by switching sides. In the logistic regression below, concerning betas are the ones for repetition and the repetition:switch interaction. If the latter is significant it ought to be negative (i.e. switched trials are harder than non-switched trials).

```{r}
tmp <- trials[trials$grid %in% trials$grid[!is.na(trials$stimulusParent)], ]
tmp$swapped <- F

m <- is.na(tmp$stimulusParent)

novel <- tmp[m, ]
novel$rep <- F
repeated <- tmp[!m, ]
repeated$rep <- T

repeated$swapped <- sapply(repeated$grid, function(g) 
  repeated$correctAnswer[repeated$grid == g] == novel$correctAnswer[novel$grid == g]
  )

tmp <- rbind(novel, repeated)

out <- lm(initialCorrect ~ rep * swapped, 
          data = aggregate(initialCorrect ~ pid + rep + swapped, data = tmp, FUN = mean))

summary(out)
```

## Type 2 performance

Type 2 (metacognitive) performance is characterised using Type 2 ROC. The mean ROC for all participants is tabulated below:

```{r results = 'asis'}
df.type2 <- NULL
for(p in unique(trials$pid)) {
  for(d in c('initial', 'final')) {
    tmp <- trials[trials$pid == p, c(paste0(d, 'Correct'), paste0(d, 'Confidence'))]
    # remove NA values which appear in final judgements which are never made
    tmp <- tmp[!is.na(tmp[ ,1]), ]
    roc <- type2ROC(tmp[ ,1], tmp[ ,2], bins = 7)
    df.type2 <- rbind(df.type2, data.frame(pid = factor(p), decision = d, conf = roc$x, pCorrect = roc$y))
  }
}
tmp <- seq(0, 1, length.out = length(unique(df.type2$conf)))
tmp <- sapply(1:(length(tmp)-1), function(i) mean(c(tmp[i], tmp[i+1])))
df.type2$confProp <- sapply(df.type2$conf, function(x) tmp[which(levels(df.type2$conf) == x)])

tmp <- aggregate(pCorrect ~ conf + decision, df.type2, mean)
# print neatly with rounding
tmp[ ,3] <- prop2str(tmp[ ,3])
kable(prop2str(tmp))
```

Participants' ROC curves:

```{r}
ggplot(df.type2, aes(x = confProp, y = pCorrect, colour = pid)) +
  geom_abline(slope = 1, intercept = c(0,0), linetype = 'dashed', colour = 'black') +
  geom_point() +
  geom_line(alpha = .5, aes(group = pid)) +
  facet_wrap(~decision, labeller = label_both) +
  scale_x_continuous(limits = c(0,1)) +
  coord_fixed() +
  style.long +
  theme(panel.spacing.x = unit(1, 'lines'))
```

# Exploration 

# Credits 

## Acknowledgements

Thanks as always to Nick Yeung and the other folks at the [ACC Lab](https://www.psy.ox.ac.uk/research/attention-cognitive-control-lab).

## R Packages

```{r results = 'asis'}
# list packages
packageNames <- (.packages())
# don't include very core package
packageNames <- packageNames[!(packageNames %in% rownames(installed.packages(priority="base")))]
# but do include the base package
packageNames <- c("base", packageNames)
out <- NULL
for(p in packageNames) {
  out <- rbind(out, data.frame('Package' = p, 
                               'Citations' = paste(format(citation(p), style = 'textVersion'), 
                                                   collapse = '<br/><br/>')))
}

kable(out)
```

## Funding

Matt Jaquiery is funded by a studentship from the [Medical Research Council](https://mrc.ukri.org/) (reference 1943590) and the University of Oxford [Department of Experimental Psychology](https://www.psy.ox.ac.uk/) (reference 17/18_MSD_661552).

## Technical details  

```{r results = 'hold'}
cat(paste('Time stamp:', Sys.time(), '\n\n'))
cat('Runtime \n')
proc.time()
cat('\n')
sessionInfo()
```