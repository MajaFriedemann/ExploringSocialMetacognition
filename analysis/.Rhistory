stat_summary(geom = 'point', size = 3, fun.y = mean, position = position_dodge(w)) +
stat_summary(geom = 'errorbar', fun.data = mean_cl_boot, position = position_dodge(w), size = 0.2) +
geom_point(alpha = 0.5, position = position_dodge(w)) +
facet_wrap(~initialCorrect, labeller = label_both) +
scale_x_discrete(name = 'Confidence Category', labels = c('Low', 'Med', 'High')) +
scale_color_discrete(name = 'Advice Type', labels = c('HighAgr', 'LowAgr')) +
scale_y_continuous(name = 'Trial Count') +
scale_shape_discrete(name = 'Advisor Agreement', labels = c('Disagree', 'Agree')) +
labs(title = 'Observed trial count for dis/agreement from each advisor by initial decision confidence\n and correctness') +
style
gg.iii.vi
# Chunk 11
participants$excluded <- sapply(participants$pid, function(pid){
ts <- which(all.trials$pid == pid)
# overall accuracy of initial decisions
v <- all.trials$initialAnswer[ts] == all.trials$correctAnswer[ts]
m <- mean(as.numeric(v), na.rm = T)
if(m < .6 | m > .85)
return('Accuracy')
# varied use of confidence scale
ts <- which(trials$pid == pid)
cCs <- aggregate(pid ~ confidenceCategory, data = trials[ts, ], FUN = length)
# All confidence categories must be used
if(nrow(cCs) < 3)
return ('Confidence')
# All confidence categories must have at least 5% of the number of trials
if(any(cCs$pid < length(ts)*.05))
return('Confidence.cat')
return(F)
})
# exclude on the basis of collecting too much data
if(sum(participants$excluded == F) > 50) {
tmp <- participants[participants$excluded == F, c('id', 'timeStart')]
tmp <- tmp$id[order(tmp$timeStart)]
tmp <- tmp[1:50]
participants$excluded[!(participants$id %in% tmp)] <- 'Excess.data'
}
all.participants <- participants
participants <- participants[participants$excluded==F, ]
# Remove excluded participants' data from other data frames
trials <- trials[trials$pid %in% participants$pid, ]
advisors <- advisors[advisors$pid %in% participants$pid, ]
questionnaires <- questionnaires[questionnaires$pid %in% participants$pid, ]
all.genTrustQ <- genTrustQ
genTrustQ <- genTrustQ[genTrustQ$pid %in% participants$pid, ]
df.iv <- aggregate(pid ~ excluded, data = all.participants, FUN = length)
names(df.iv) <- c('exclusionReason', 'count')
kable(df.iv)
# Chunk 12
df.v.i <- NULL
for(col in c('initial', 'final')) {
for(aT in c(unique(trials$adviceType),adviceTypes$neutral)) {
colName <- paste0(col,'Correct')
if(aT==adviceTypes$neutral)
aT <- adviceTypes # hack for including the total
# aggregate to get the proportion correct for the current set of interest
x <- aggregate(trials[trials$adviceType %in% aT, c(colName,'pid','adviceType')],
by = list(trials$pid[trials$adviceType %in% aT]),
FUN = function(x){sum(as.numeric(x))/length(x)})[,colName]
cl <- mean_cl_normal(x)
rn <- range(x)
df.v.i <- rbind(df.v.i, data.frame(decision = col,
adviceType = ifelse(length(aT)>1,'Total',getAdviceTypeName(aT)), # hack to label total
target = ifelse(col=='initial',.71,NA),
meanCorrect = cl$y,
cl95Min = cl$ymin,
cl95Max = cl$ymax,
rangeMin = rn[1],
rangeMax = rn[2]))
}
}
df.v.i[,-(1:2)] <- round(df.v.i[,-(1:2)],2)
kable(df.v.i)
# Chunk 13
df.v.ii <- NULL
for(aT in unique(trials$adviceType)) {
ts <- trials[trials$adviceType==aT, ]
for(i in 1:(length(confidenceCategories)+3)) {
if(i <= length(confidenceCategories)) {
x <- ts[ts$confidenceCategory==confidenceCategories[i] & ts$initialCorrect==T, ]
name <- names(confidenceCategories[i])
} else {
i <- i - length(confidenceCategories)
if(i==1) {
x <- ts[ts$initialCorrect, ]
name <- 'allCorrect'
} else if(i==2) {
x <- ts[!ts$initialCorrect, ]
name <- 'allWrong'
} else {
x <- ts
name <- 'All'
}
}
x <- aggregate(advisorAgrees ~ pid, data = x,
FUN = function(x){sum(as.numeric(x))/length(x)})$advisorAgrees
cl <- mean_cl_normal(x)
rn <- range(x)
df.v.ii <- rbind(df.v.ii, data.frame(adviceType = getAdviceTypeName(aT), # hack to label totalname,
name,
probAgree = cl$y,
cl95Min = cl$ymin,
cl95Max = cl$ymax,
rangeMin = rn[1],
rangeMax = rn[2]))
}
}
df.v.ii[,-(1:2)] <- round(df.v.ii[,-(1:2)],2)
kable(df.v.ii)
# Chunk 14
df.v.iii.i <- NULL
for(col in c('initial', 'final')) {
for(correct in list(T,F,c(T,F))) {
colName <- paste0(col,'Confidence')
x <- aggregate(trials[trials[,paste0(col,'Correct')] %in% correct, c(colName,'pid','adviceType')],
by = list(trials$pid[trials[,paste0(col,'Correct')] %in% correct]),
FUN = function(x){sum(as.numeric(x))/length(x)})[,colName]
cl <- mean_cl_normal(x)
rn <- range(x)
df.v.iii.i <- rbind(df.v.iii.i, data.frame(decision = col,
correct = ifelse(length(correct)>1,'Both',correct), # hack to label total
meanConfidence = cl$y,
cl95Min = cl$ymin,
cl95Max = cl$ymax,
rangeMin = rn[1],
rangeMax = rn[2]))
}
}
df.v.iii.i[,-(1:2)] <- round(df.v.iii.i[,-(1:2)],2)
kable(df.v.iii.i)
# Chunk 15
df.v.iii.ii <- NULL
for(col in c('initial', 'final')) {
for(aT in c(unique(trials$adviceType), adviceTypes$neutral)) {
if(aT==adviceTypes$neutral)
aT <- adviceTypes # hack for including the total
colName <- paste0(col,'Confidence')
x <- aggregate(trials[trials[,"adviceType"] %in% aT, c(colName,'pid','adviceType')],
by = list(trials$pid[trials[,"adviceType"] %in% aT]),
FUN = function(x){sum(as.numeric(x))/length(x)})[,colName]
cl <- mean_cl_normal(x)
rn <- range(x)
df.v.iii.ii <- rbind(df.v.iii.ii, data.frame(decision = col,
adviceType = ifelse(length(aT)>1,'Both',getAdviceTypeName(aT)), # hack to label total
meanConfidence = cl$y,
cl95Min = cl$ymin,
cl95Max = cl$ymax,
rangeMin = rn[1],
rangeMax = rn[2]))
}
}
df.v.iii.ii[,-(1:2)] <- round(df.v.iii.ii[,-(1:2)],2)
kable(df.v.iii.ii)
# Chunk 16
df.v.iii.2 <- NULL
for(agree in c(T,F)) {
for(aT in c(unique(trials$adviceType), adviceTypes$neutral)) {
if(aT==adviceTypes$neutral)
aT <- adviceTypes # hack for including the total
x <- aggregate(finalConfidence ~ pid,
data = trials[trials$advisorAgrees==agree & trials$adviceType %in% aT, ],
FUN = function(x){sum(as.numeric(x))/length(x)})$finalConfidence
cl <- mean_cl_normal(x)
rn <- range(x)
df.v.iii.2 <- rbind(df.v.iii.2, data.frame(agree,
advisor = ifelse(length(aT)>1,'Both',getAdviceTypeName(aT)), # hack to label total
meanConfidence = cl$y,
cl95Min = cl$ymin,
cl95Max = cl$ymax,
rangeMin = rn[1],
rangeMax = rn[2]))
}
}
df.v.iii.2[,-(1:2)] <- round(df.v.iii.2[,-(1:2)],2)
kable(df.v.iii.2)
# Chunk 17
df.poly1 <- data.frame(    # These polygon points define a parellelogram marking the limits for the capped influence
x=c(50, 0, 0),
y=c(50, 50, -50)
)
df.poly2 <- df.poly1 * -1
gg.v.iv <- ggplot(trials, aes(x = initialConfSpan, y = finalConfSpan)) +
geom_polygon(data = df.poly1, aes(x,y), fill = 'grey', alpha = 0.2) +
geom_polygon(data = df.poly2, aes(x,y), fill = 'grey', alpha = 0.2) +
geom_point(alpha = 0.2, aes(color = factor(finalCorrect))) +
geom_abline(slope = 1, intercept = 0, linetype = 'dashed', size = 1, color = 'black') +
scale_color_discrete(name = 'Final judgement', labels = c('Incorrect', 'Correct')) +
scale_x_continuous(limits = c(-50,50), expand = c(0,0)) +
scale_y_continuous(limits = c(-50,50), expand = c(0,0)) +
facet_grid(~advisorAgrees, labeller = as_labeller(c('FALSE'='Disagree', 'TRUE'='Agree'))) +
labs(title = "Initial vs final confidence",
legend = NULL,
x = 'Initial confidence',
y = "Final confidence") +
coord_fixed() +
style +
theme(panel.spacing = unit(1.5, 'lines'),
plot.margin = unit(c(0,1,0,0.5), 'lines'))
gg.v.iv
# Chunk 18
df.v.iv <- NULL
for(a in c(T,F)) {
v <- trials[trials$advisorAgrees == a, ]
i <- mean(v$confidenceShiftRaw > 0)
d <- mean(v$confidenceShiftRaw < 0)
n <- mean(v$confidenceShiftRaw == 0)
df.v.iv <- rbind(df.v.iv, data.frame(advisorAgrees = a,
increaseConfPerC = i,
noChangePerC = n,
decreaseConfPerC = d))
}
kable(df.v.iv)
df.v.iv.2 <- NULL
for(a in c(T,F)) {
for(pid in unique(trials$pid)) {
v <- trials[trials$advisorAgrees==a & trials$pid == pid, ]
i <- mean(v$confidenceShiftRaw > 0)
d <- mean(v$confidenceShiftRaw < 0)
n <- mean(v$confidenceShiftRaw == 0)
df.v.iv.2 <- rbind(df.v.iv.2, data.frame(pid, advisorAgrees = a,
increasConfPerC = i,
noChangePerC = n,
decreaseConfPerC = d,
totalN = nrow(v)))
}
}
#kable(df.v.iv.2)
#kable(aggregate(rawInfluence ~ pid + advisorAgrees, data = trials, FUN = mean))
# Chunk 19
df.v.v <- NULL
for(aT in unique(trials$adviceType)) {
for(tp in unique(questionnaires$timepoint)) {
for(colName in c('likeability', 'ability', 'benevolence')) {
x <- aggregate(questionnaires[questionnaires$adviceType==aT & questionnaires$timepoint==tp, c('pid',colName)],
by = list(questionnaires$pid[questionnaires$adviceType==aT
& questionnaires$timepoint==tp]),
FUN = function(x){sum(as.numeric(x))/length(x)})[,colName]
cl <- mean_cl_normal(x)
rn <- range(x)
df.v.v <- rbind(df.v.v, data.frame(timepoint = tp,
question = colName,
adviceType = getAdviceTypeName(aT),
mean = cl$y,
cl95Min = cl$ymin,
cl95Max = cl$ymax,
rangeMin = rn[1],
rangeMax = rn[2]))
}
}
}
df.v.v[,-(1:3)] <- round(df.v.v[,-(1:3)],2)
kable(df.v.v)
# Chunk 20
df.vi <- NULL
for(agree in list(T,F,c(T,F))) {
for(aT in c(unique(trials$adviceType), adviceTypes$neutral)) {
if(aT==adviceTypes$neutral)
aT <- adviceTypes # hack for including the total
tmp <- trials[trials$advisorAgrees %in% agree & trials$adviceType %in% aT, ]
tmp$adviceCorrect <- tmp$adviceSide == tmp$correctAnswer
x <- aggregate(adviceCorrect ~ pid,
tmp,
FUN = mean)$adviceCorrect
cl <- mean_cl_normal(x)
rn <- range(x)
df.vi <- rbind(df.vi, data.frame(agree = ifelse(length(agree)>1, 'Both', agree),
advisor = ifelse(length(aT)>1,'Both',getAdviceTypeName(aT)), # hack to label total
meanAccuracy = cl$y,
cl95Min = cl$ymin,
cl95Max = cl$ymax,
rangeMin = rn[1],
rangeMax = rn[2]))
}
}
df.vi[,-(1:2)] <- round(df.vi[,-(1:2)],2)
kable(df.vi)
# Chunk 21
df.vii <- NULL
tmp <- aggregate(id ~ confidenceCategory + pid, trials, length)
for(cc in confidenceCategories) {
v <- tmp$id[tmp$confidenceCategory==cc]
cl <- mean_cl_normal(v)
rn <- range(v)
df.vii <- rbind(df.vii, data.frame(confidenceCategory = cc,
meanN = cl$y,
cl95L = cl$ymin,
cl95H = cl$ymax,
rangeL = rn[1],
rangeH = rn[2]))
}
#kable(round(df.vii,2))
df.vii.2 <- NULL
tmp <- NULL
for(pid in unique(trials$pid)) {
v <- trials$confidenceCategory[trials$pid==pid]
tmp <- rbind(tmp, data.frame(pid,
low=mean(v==confidenceCategories$low),
med=mean(v==confidenceCategories$medium),
high=mean(v==confidenceCategories$high)))
}
for(cc in confidenceCategories) {
v <- tmp[ ,2+cc] # for each confidence category in tmp
cl <- mean_cl_normal(v)
rn <- range(v)
df.vii.2 <- rbind(df.vii.2, data.frame(confidenceCategory = cc,
meanProp = cl$y,
cl95L = cl$ymin, cl95H = cl$ymax,
rangeL = rn[1], rangeH = rn[2]))
}
#kable(round(df.vii.2,2))
# For parity with MATLAB experiment we look at confidence categories for initially-correct trials only
df.vii.3 <- NULL
tmp <- NULL
for(pid in unique(trials$pid)) {
v <- trials$confidenceCategory[trials$pid==pid]
v[!trials$initialCorrect[trials$pid==pid]] <- NaN # set non-initially correct to NaN
tmp <- rbind(tmp, data.frame(pid,
low=mean(v==confidenceCategories$low, na.rm=T),
med=mean(v==confidenceCategories$medium, na.rm=T),
high=mean(v==confidenceCategories$high, na.rm=T),
nan=mean(is.nan(v))))
}
for(cc in c(confidenceCategories,NaN)) {
v <- tmp[ ,2+ifelse(is.nan(cc),3,cc)] # for each confidence category in tmp
cl <- mean_cl_normal(v)
rn <- range(v)
df.vii.3 <- rbind(df.vii.3, data.frame(confidenceCategory = cc,
meanProp = cl$y,
cl95L = cl$ymin, cl95H = cl$ymax,
rangeL = rn[1], rangeH = rn[2]))
}
kable(prop2str(df.vii.3,2))
df.v.viii <- NULL
df.v.viii.tests <- NULL
# decision accuracy
df.v.viii <- rbind(df.v.viii,
cbind(data.frame(labels = c('initial', 'final')), # labels
df.v.i[c(3,6), ])) # data
kable(prop2str(df.v.viii))
kable(df.v.viii)
# decision accuracy
df.v.viii <- rbind(df.v.viii,
cbind(data.frame(labels = rep('accuracy', 2)), # label
df.v.i[c(3,6), ])) # data
kable(df.v.viii)
df.v.viii <- NULL
# decision accuracy
df.v.viii <- rbind(df.v.viii,
cbind(data.frame(labels = rep('accuracy', 2)), # label
df.v.i[c(3,6), ])) # data
kable(df.v.viii)
df.v.viii <- NULL
# decision accuracy
df.v.viii <- rbind(df.v.viii,
cbind(data.frame(variable = rep('accuracy', 2)), # label
df.v.i[c(3,6), ])) # data
kable(df.v.viii)
df.v.iii.ii
df.v.iii.i
df.v.viii <- NULL
# decision accuracy
df.v.viii <- rbind(df.v.viii,
cbind(data.frame(variable = rep('accuracy', 2)), # label
df.v.i[c(3,6), ])) # data
# decision confidence
df.v.viii <- rbind(df.v.viii,
cbind(data.frame(variable = rep('confidence', 2)), # label
df.v.iii.i[c(3,6), ])) # data
# confidence resolution
# confidence calibration
# confidence brier score
kable(df.v.viii)
df.v.viii.tests <- NULL
# t-test before/after on accuracy
# t-test before/after on confidence
# t-test before/after on resolution
# t-test before/after on calibration
# t-test before/after on brier score
cbind(data.frame(variable = rep('confidence', 2)), # label
df.v.iii.i[c(3,6), ])
# decision accuracy
tmp <- cbind(data.frame(variable = rep('accuracy', 2)), # label
df.v.i[c(3,6), ]) # data
aggregate(trials[ , c(colName,'pid')],
by = list(trials$pid),
FUN = function(x){sum(as.numeric(x))/length(x)})
aggregate(trials[ , c('initialCorrect', 'pid')],
by = list(trials$pid),
FUN = function(x){sum(as.numeric(x))/length(x)})
aggregate(trials[ , c(colName, 'pid')],
by = list('pid' = trials$pid),
FUN = function(x){sum(as.numeric(x))/length(x)})
aggregate(trials[ , c('initialCorrect', 'pid')],
by = list('pid' = trials$pid),
FUN = function(x){sum(as.numeric(x))/length(x)})
tmp <- aggregate(trials[ , c('initialCorrect', 'finalCorrect')],
by = list('pid' = trials$pid),
FUN = function(x){sum(as.numeric(x))/length(x)})
kable(tmp)
ttestBF(tmp$initialCorrect, tmp$finalCorrect, paired = T)
t.test(tmp$initialCorrect, tmp$finalCorrect, paired = T)
prettyPrint(t.test(tmp$initialCorrect, tmp$finalCorrect, paired = T))
ttestBF(tmp$initialCorrect, tmp$finalCorrect, paired = T)
ttestBF(tmp$initialCorrect, tmp$finalCorrect)
ttestBF(tmp$initialCorrect, tmp$finalCorrect, paired = T)
prop2str(0.003)
# Print the results of a t-test as we would like to see them reported in a paper
prettyPrint <- function(results, d = NULL) {
es <- NULL
if(!is.null(d))
es <- paste0(' , d=', round(d,2))
print(paste0('t(',results$parameter,')=',round(results$statistic,2),
' [',round(attr(results$conf.int, "conf.level")*100),'%CI: ',
round(results$conf.int[[1]],2), ', ', round(results$conf.int[[2]],2),'],',
' p=',prop2str(results$p.value,3), es))
}
# decision accuracy
tmp <- cbind(data.frame(variable = rep('accuracy', 2)), # label
df.v.i[c(3,6), ]) # data
kable(tmp)
tmp <- aggregate(trials[ , c('initialCorrect', 'finalCorrect')],
by = list('pid' = trials$pid),
FUN = function(x){sum(as.numeric(x))/length(x)})
ttestBF(tmp$initialCorrect, tmp$finalCorrect, paired = T)
prettyPrint(t.test(tmp$initialCorrect, tmp$finalCorrect, paired = T))
# decision confidence
df.v.viii <- rbind(df.v.viii,
cbind(data.frame(variable = rep('confidence', 2)), # label
df.v.iii.i[c(3,6), ])) # data
# confidence resolution
# confidence calibration
# confidence brier score
kable(df.v.viii)
df.v.viii.tests <- NULL
# t-test before/after on accuracy
# t-test before/after on confidence
# t-test before/after on resolution
# t-test before/after on calibration
# t-test before/after on brier score
prettyPrint(t.test(tmp$initialCorrect, tmp$finalCorrect, paired = T))
prettyPrint(t.test(tmp$initialCorrect, tmp$finalCorrect, paired = T))
# decision accuracy
tmp <- cbind(data.frame(variable = rep('accuracy', 2)), # label
df.v.i[c(3,6), ]) # data
kable(tmp)
tmp <- aggregate(trials[ , c('initialCorrect', 'finalCorrect')],
by = list('pid' = trials$pid),
FUN = function(x){sum(as.numeric(x))/length(x)})
ttestBF(tmp$initialCorrect, tmp$finalCorrect, paired = T)
prettyPrint(t.test(tmp$initialCorrect, tmp$finalCorrect, paired = T))
# decision confidence
df.v.viii <- rbind(df.v.viii,
cbind(data.frame(variable = rep('confidence', 2)), # label
df.v.iii.i[c(3,6), ])) # data
# confidence resolution
# confidence calibration
# confidence brier score
kable(df.v.viii)
df.v.viii.tests <- NULL
# t-test before/after on accuracy
# t-test before/after on confidence
# t-test before/after on resolution
# t-test before/after on calibration
# t-test before/after on brier score
ttestBF(tmp$initialCorrect, tmp$finalCorrect, paired = T)
print(prettyPrint(t.test(tmp$initialCorrect, tmp$finalCorrect, paired = T)))
# decision accuracy
tmp <- cbind(data.frame(variable = rep('accuracy', 2)), # label
df.v.i[c(3,6), ]) # data
kable(tmp)
tmp <- aggregate(trials[ , c('initialCorrect', 'finalCorrect')],
by = list('pid' = trials$pid),
FUN = function(x){sum(as.numeric(x))/length(x)})
ttestBF(tmp$initialCorrect, tmp$finalCorrect, paired = T)
print(prettyPrint(t.test(tmp$initialCorrect, tmp$finalCorrect, paired = T)))
# decision confidence
df.v.viii <- rbind(df.v.viii,
cbind(data.frame(variable = rep('confidence', 2)), # label
df.v.iii.i[c(3,6), ])) # data
# confidence resolution
# confidence calibration
# confidence brier score
kable(df.v.viii)
df.v.viii.tests <- NULL
# t-test before/after on accuracy
# t-test before/after on confidence
# t-test before/after on resolution
# t-test before/after on calibration
# t-test before/after on brier score
# decision accuracy
tmp <- cbind(data.frame(variable = rep('accuracy', 2)), # label
df.v.i[c(3,6), ]) # data
kable(tmp)
tmp <- aggregate(trials[ , c('initialCorrect', 'finalCorrect')],
by = list('pid' = trials$pid),
FUN = function(x){sum(as.numeric(x))/length(x)})
ttestBF(tmp$initialCorrect, tmp$finalCorrect, paired = T)
print(prettyPrint(t.test(tmp$initialCorrect, tmp$finalCorrect, paired = T)))
# decision confidence
df.v.viii <- rbind(df.v.viii,
cbind(data.frame(variable = rep('confidence', 2)), # label
df.v.iii.i[c(3,6), ])) # data
# confidence resolution
# confidence calibration
# confidence brier score
kable(df.v.viii)
df.v.viii.tests <- NULL
# t-test before/after on accuracy
# t-test before/after on confidence
# t-test before/after on resolution
# t-test before/after on calibration
# t-test before/after on brier score
