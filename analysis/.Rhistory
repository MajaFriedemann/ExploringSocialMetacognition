FUN = function(x){sum(as.numeric(x))/length(x)})[,colName]
cl <- mean_cl_normal(x)
rn <- range(x)
df.v.i <- rbind(df.v.i, data.frame(decision = col,
adviceType = ifelse(length(aT)>1,'Total',getAdviceTypeName(aT)), # hack to label total
target = ifelse(col=='initial',.71,NA),
meanCorrect = cl$y,
cl95Min = cl$ymin,
cl95Max = cl$ymax,
rangeMin = rn[1],
rangeMax = rn[2]))
}
}
df.v.i[,-(1:2)] <- round(df.v.i[,-(1:2)],2)
kable(df.v.i)
# Chunk 11
df.v.ii <- NULL
for(aT in c(5,6)) {
if(aT==adviceTypes$neutral)
next()
ts <- trials[trials$adviceType==aT, ]
for(i in 1:(length(confidenceCategories)+3)) {
if(i <= length(confidenceCategories)) {
x <- ts[ts$confidenceCategory==confidenceCategories[i] & ts$initialCorrect==T, ]
name <- names(confidenceCategories[i])
} else {
i <- i - length(confidenceCategories)
if(i==1) {
x <- ts[ts$initialCorrect, ]
name <- 'allCorrect'
} else if(i==2) {
x <- ts[!ts$initialCorrect, ]
name <- 'allWrong'
} else {
x <- ts
name <- 'All'
}
}
x <- aggregate(advisorAgrees ~ pid, data = x,
FUN = function(x){sum(as.numeric(x))/length(x)})$advisorAgrees
cl <- mean_cl_normal(x)
rn <- range(x)
df.v.ii <- rbind(df.v.ii, data.frame(adviceType = getAdviceTypeName(aT), # hack to label totalname,
name,
probAgree = cl$y,
cl95Min = cl$ymin,
cl95Max = cl$ymax,
rangeMin = rn[1],
rangeMax = rn[2]))
}
}
df.v.ii[,-(1:2)] <- round(df.v.ii[,-(1:2)],2)
kable(df.v.ii)
# Chunk 12
df.v.iii.i <- NULL
for(col in c('initial', 'final')) {
for(correct in list(T,F,c(T,F))) {
colName <- paste0(col,'Confidence')
x <- aggregate(trials[trials[,paste0(col,'Correct')] %in% correct, c(colName,'pid','adviceType')],
by = list(trials$pid[trials[,paste0(col,'Correct')] %in% correct]),
FUN = function(x){sum(as.numeric(x))/length(x)})[,colName]
cl <- mean_cl_normal(x)
rn <- range(x)
df.v.iii.i <- rbind(df.v.iii.i, data.frame(decision = col,
correct = ifelse(length(correct)>1,'Both',correct), # hack to label total
meanConfidence = cl$y,
cl95Min = cl$ymin,
cl95Max = cl$ymax,
rangeMin = rn[1],
rangeMax = rn[2]))
}
}
df.v.iii.i[,-(1:2)] <- round(df.v.iii.i[,-(1:2)],2)
kable(df.v.iii.i)
# Chunk 13
df.v.iii.ii <- NULL
for(col in c('initial', 'final')) {
for(aT in c(adviceTypes$HighAcc, adviceTypes$LowAcc, adviceTypes$neutral)) {
if(aT==adviceTypes$neutral)
aT <- c(adviceTypes$HighAcc, adviceTypes$LowAcc) # hack for including the total
colName <- paste0(col,'Confidence')
x <- aggregate(trials[trials[,"adviceType"] %in% aT, c(colName,'pid','adviceType')],
by = list(trials$pid[trials[,"adviceType"] %in% aT]),
FUN = function(x){sum(as.numeric(x))/length(x)})[,colName]
cl <- mean_cl_normal(x)
rn <- range(x)
df.v.iii.ii <- rbind(df.v.iii.ii, data.frame(decision = col,
adviceType = ifelse(length(aT)>1,'Both',getAdviceTypeName(aT)), # hack to label total
meanConfidence = cl$y,
cl95Min = cl$ymin,
cl95Max = cl$ymax,
rangeMin = rn[1],
rangeMax = rn[2]))
}
}
df.v.iii.ii[,-(1:2)] <- round(df.v.iii.ii[,-(1:2)],2)
kable(df.v.iii.ii)
# Chunk 14
df.v.iii.2 <- NULL
for(agree in c(T,F)) {
for(aT in c(adviceTypes$HighAcc, adviceTypes$LowAcc, adviceTypes$neutral)) {
if(aT==adviceTypes$neutral)
aT <- c(adviceTypes$HighAcc, adviceTypes$LowAcc) # hack for including the total
x <- aggregate(finalConfidence ~ pid,
data = trials[trials$advisorAgrees==agree & trials$adviceType %in% aT, ],
FUN = function(x){sum(as.numeric(x))/length(x)})$finalConfidence
cl <- mean_cl_normal(x)
rn <- range(x)
df.v.iii.2 <- rbind(df.v.iii.2, data.frame(agree,
advisor = ifelse(length(aT)>1,'Both',getAdviceTypeName(aT)), # hack to label total
meanConfidence = cl$y,
cl95Min = cl$ymin,
cl95Max = cl$ymax,
rangeMin = rn[1],
rangeMax = rn[2]))
}
}
df.v.iii.2[,-(1:2)] <- round(df.v.iii.2[,-(1:2)],2)
kable(df.v.iii.2)
# Chunk 15
df.poly1 <- data.frame(    # These polygon points define a parellelogram marking the limits for the capped influence
x=c(50, 0, 0),
y=c(50, 50, -50)
)
df.poly2 <- df.poly1 * -1
gg.v.iv <- ggplot(trials, aes(x = initialConfSpan, y = finalConfSpan)) +
geom_polygon(data = df.poly1, aes(x,y), fill = 'grey', alpha = 0.2) +
geom_polygon(data = df.poly2, aes(x,y), fill = 'grey', alpha = 0.2) +
geom_point(alpha = 0.2, aes(color = factor(finalCorrect))) +
geom_abline(slope = 1, intercept = 0, linetype = 'dashed', size = 1, color = 'black') +
scale_color_discrete(name = 'Final judgement', labels = c('Incorrect', 'Correct')) +
scale_x_continuous(limits = c(-50,50), expand = c(0,0)) +
scale_y_continuous(limits = c(-50,50), expand = c(0,0)) +
facet_grid(~advisorAgrees, labeller = as_labeller(c('FALSE'='Disagree', 'TRUE'='Agree'))) +
labs(title = "Initial vs final confidence",
legend = NULL,
x = 'Initial confidence',
y = "Final confidence") +
coord_fixed() +
style +
theme(panel.spacing = unit(1.5, 'lines'),
plot.margin = unit(c(0,1,0,0.5), 'lines'))
gg.v.iv
# Chunk 16
df.v.iv <- NULL
for(a in c(T,F)) {
v <- trials[trials$advisorAgrees == a, ]
i <- mean(v$confidenceShiftRaw > 0)
d <- mean(v$confidenceShiftRaw < 0)
n <- mean(v$confidenceShiftRaw == 0)
df.v.iv <- rbind(df.v.iv, data.frame(advisorAgrees = a,
increaseConfPerC = i,
noChangePerC = n,
decreaseConfPerC = d))
}
kable(df.v.iv)
df.v.iv.2 <- NULL
for(a in c(T,F)) {
for(pid in unique(trials$pid)) {
v <- trials[trials$advisorAgrees==a & trials$pid == pid, ]
i <- mean(v$confidenceShiftRaw > 0)
d <- mean(v$confidenceShiftRaw < 0)
n <- mean(v$confidenceShiftRaw == 0)
df.v.iv.2 <- rbind(df.v.iv.2, data.frame(pid, advisorAgrees = a,
increasConfPerC = i,
noChangePerC = n,
decreaseConfPerC = d,
totalN = nrow(v)))
}
}
#kable(df.v.iv.2)
#kable(aggregate(rawInfluence ~ pid + advisorAgrees, data = trials, FUN = mean))
# Chunk 17
df.v.v <- NULL
for(aT in c(5,6)) {
if(aT == adviceTypes$neutral)
next()
for(tp in unique(questionnaires$timepoint)) {
for(colName in c('likeability', 'ability', 'benevolence')) {
x <- aggregate(questionnaires[questionnaires$adviceType==aT & questionnaires$timepoint==tp, c('pid',colName)],
by = list(questionnaires$pid[questionnaires$adviceType==aT
& questionnaires$timepoint==tp]),
FUN = function(x){sum(as.numeric(x))/length(x)})[,colName]
cl <- mean_cl_normal(x)
rn <- range(x)
df.v.v <- rbind(df.v.v, data.frame(timepoint = tp,
question = colName,
adviceType = getAdviceTypeName(aT),
mean = cl$y,
cl95Min = cl$ymin,
cl95Max = cl$ymax,
rangeMin = rn[1],
rangeMax = rn[2]))
}
}
}
df.v.v[,-(1:3)] <- round(df.v.v[,-(1:3)],2)
kable(df.v.v)
# Chunk 18
df.vi <- NULL
for(agree in list(T,F,c(T,F))) {
for(aT in c(adviceTypes$HighAcc, adviceTypes$LowAcc, adviceTypes$neutral)) {
if(aT==adviceTypes$neutral)
aT <- c(adviceTypes$HighAcc, adviceTypes$LowAcc) # hack for including the total
tmp <- trials[trials$advisorAgrees %in% agree & trials$adviceType %in% aT, ]
tmp$adviceCorrect <- tmp$adviceSide == tmp$correctAnswer
x <- aggregate(adviceCorrect ~ pid,
tmp,
FUN = mean)$adviceCorrect
cl <- mean_cl_normal(x)
rn <- range(x)
df.vi <- rbind(df.vi, data.frame(agree = ifelse(length(agree)>1, 'Both', agree),
advisor = ifelse(length(aT)>1,'Both',getAdviceTypeName(aT)), # hack to label total
meanAccuracy = cl$y,
cl95Min = cl$ymin,
cl95Max = cl$ymax,
rangeMin = rn[1],
rangeMax = rn[2]))
}
}
df.vi[,-(1:2)] <- round(df.vi[,-(1:2)],2)
kable(df.vi)
# Chunk 19
df.vii <- NULL
tmp <- aggregate(id ~ confidenceCategory + pid, trials, length)
for(cc in confidenceCategories) {
v <- tmp$id[tmp$confidenceCategory==cc]
cl <- mean_cl_normal(v)
rn <- range(v)
df.vii <- rbind(df.vii, data.frame(confidenceCategory = cc,
meanN = cl$y,
cl95L = cl$ymin,
cl95H = cl$ymax,
rangeL = rn[1],
rangeH = rn[2]))
}
#kable(round(df.vii,2))
df.vii.2 <- NULL
tmp <- NULL
for(pid in unique(trials$pid)) {
v <- trials$confidenceCategory[trials$pid==pid]
tmp <- rbind(tmp, data.frame(pid,
low=mean(v==confidenceCategories$low),
med=mean(v==confidenceCategories$medium),
high=mean(v==confidenceCategories$high)))
}
for(cc in confidenceCategories) {
v <- tmp[ ,2+cc] # for each confidence category in tmp
cl <- mean_cl_normal(v)
rn <- range(v)
df.vii.2 <- rbind(df.vii.2, data.frame(confidenceCategory = cc,
meanProp = cl$y,
cl95L = cl$ymin, cl95H = cl$ymax,
rangeL = rn[1], rangeH = rn[2]))
}
#kable(round(df.vii.2,2))
# For parity with MATLAB experiment we look at confidence categories for initially-correct trials only
df.vii.3 <- NULL
tmp <- NULL
for(pid in unique(trials$pid)) {
v <- trials$confidenceCategory[trials$pid==pid]
v[!trials$initialCorrect[trials$pid==pid]] <- NaN # set non-initially correct to NaN
tmp <- rbind(tmp, data.frame(pid,
low=mean(v==confidenceCategories$low, na.rm=T),
med=mean(v==confidenceCategories$medium, na.rm=T),
high=mean(v==confidenceCategories$high, na.rm=T),
nan=mean(is.nan(v))))
}
for(cc in c(confidenceCategories,NaN)) {
v <- tmp[ ,2+ifelse(is.nan(cc),3,cc)] # for each confidence category in tmp
cl <- mean_cl_normal(v)
rn <- range(v)
df.vii.3 <- rbind(df.vii.3, data.frame(confidenceCategory = cc,
meanProp = cl$y,
cl95L = cl$ymin, cl95H = cl$ymax,
rangeL = rn[1], rangeH = rn[2]))
}
kable(prop2str(df.vii.3,2))
# Chunk 20
# Plot function, expects melted dataframe
getPlot.v.viii <- function(df) {
ggplot(df, aes(x = variable, y = value, colour = as.factor(pid))) +
geom_violin(alpha = 0.25, colour = NA, fill = 'grey') +
stat_summary(geom = 'point', fun.y = mean, size = 5, shape = 23, fill = 'black', aes(group = variable)) +
stat_summary(geom = 'errorbar', fun.data = mean_cl_boot, aes(group = variable), width = 0.25) +
stat_summary(geom = 'line', fun.y = mean, aes(group = 1)) +
geom_point(alpha = 0.25) +
geom_line(alpha = 0.25, aes(group = as.factor(pid)))+
scale_x_discrete(name = '', labels = c('Initial Decision', 'Final Decision')) +
style.long
}
# decision accuracy
print('Decision accuracy:')
tmp <- aggregate(cbind(initialCorrect, finalCorrect) ~ pid, data = trials, FUN = mean)
tmp.2 <- quickCompareVectors(tmp$initialCorrect, tmp$finalCorrect, 'Initial', 'Final', paired = T)
gg.v.viii.1 <- getPlot.v.viii(melt(tmp, id.vars = 'pid')) +
scale_y_continuous(name = 'P(Correct)', expand = c(0.1,0))
gg.v.viii.1
# decision confidence
print('Decision confidence:')
tmp <- aggregate(cbind(initialConfidence, finalConfidence) ~ pid, data = trials, FUN = mean)
tmp.2 <- quickCompareVectors(tmp$initialConfidence, tmp$finalConfidence, 'Initial', 'Final', paired = T)
gg.v.viii.2 <- getPlot.v.viii(melt(tmp, id.vars = 'pid')) +
scale_y_continuous(name = 'Confidence', limits = c(0,50))
gg.v.viii.2
# metacognitive performance measures
df.metacog <- NULL
for(pid in unique(trials$pid)) {
ts <- trials[trials$pid == pid, ]
corInitial = cor(ts$initialCorrect, ts$initialConfidence)
corFinal = cor(ts$finalCorrect, ts$finalConfidence)
brierInitial = brierscore(initialCorrect ~ initialConfidence, data = ts, group = 'pid')$brieravg
brierFinal = brierscore(finalCorrect ~ finalConfidence, data = ts, group = 'pid')$brieravg
df.metacog <- rbind(df.metacog, data.frame(pid, corInitial, brierInitial, corFinal, brierFinal))
}
# confidence-accuracy correlation
print('Confidence-accuracy correlation:')
tmp.2 <- quickCompareVectors(df.metacog$corInitial, df.metacog$corFinal, 'Initial', 'Final', paired = T)
gg.v.viii.3 <- getPlot.v.viii(melt(df.metacog[ , c('pid', 'corInitial', 'corFinal')], id.vars = 'pid')) +
scale_y_continuous(name = 'Confidence-accuracy correlation', expand = c(0.1,0))
gg.v.viii.3
# confidence brier score
print('Brier score:')
tmp.2 <- quickCompareVectors(df.metacog$brierInitial, df.metacog$brierFinal, 'Initial', 'Final', paired = T)
gg.v.viii.4 <- getPlot.v.viii(melt(df.metacog[ , c('pid', 'brierInitial', 'brierFinal')], id.vars = 'pid')) +
scale_y_continuous(name = 'Brier score', expand = c(0.1,0))
gg.v.viii.4
# Chunk 21
tmp <- aggregate(adviceType ~ pid,
data = trials[trials$type==trialTypes$choice, ],
FUN = function(x)sum(x==adviceTypes$HighAcc)/length(x))
t.vi.i <- t.test(tmp$adviceType, mu=0.5)
d <- cohensD(tmp$adviceType, mu = 0.5)
tB.vi.i <- ttestBF(tmp$adviceType, mu = 0.5)
printMean(tmp$adviceType, 'Mean HighAcc pick rate', isProportion = T)
print('Choice proportion HighAcc vs. chance level (.5)')
prettyPrint(t.vi.i, d)
print('Bayesian examination of above (prior = mean of 0.5, sd as empirically observed)')
print(tB.vi.i)
print(paste0('Evidence strength for preferential HighAcc picking: BF=', round(exp(tB.vi.i@bayesFactor$bf),3)))
# Chunk 22
tmp <- aggregate(adviceType ~ pid,
data = trials[trials$type==trialTypes$choice
& trials$confidenceCategory==confidenceCategories$medium, ],
FUN = function(x)sum(x==adviceTypes$HighAcc)/length(x))
t.vi.ii <- t.test(tmp$adviceType, mu=0.5)
d <- cohensD(tmp$adviceType, mu=0.5)
tB.vi.ii <- ttestBF(tmp$adviceType, mu = 0.5)
printMean(tmp$adviceType, 'Mean HighAcc pick rate', isProportion = T)
print('Choice proportion HighAcc vs. chance level (.5) for mid-confidence trials')
prettyPrint(t.vi.ii,d)
print('Bayesian examination of above (prior = mean of 0.5, sd as empirically observed)')
print(tB.vi.ii)
print(paste0('Evidence strength for preferential HighAcc picking: BF=', round(exp(tB.vi.ii@bayesFactor$bf),3)))
# Chunk 23
tmp <- aggregate(adviceType ~ pid + confidenceCategory,
data = trials[trials$type==trialTypes$choice, ],
FUN = function(x)sum(x==adviceTypes$HighAcc)/length(x))
tmp.2 <- aggregate(adviceType ~ pid,
data = trials[trials$type == trialTypes$choice, ],
FUN = function(x)sum(x==adviceTypes$HighAcc)/length(x))
gg.vi.iii <- ggplot(tmp, aes(x = factor(confidenceCategory), y = adviceType)) +
# Reference line
geom_hline(linetype = "dashed", color = "black", yintercept = .5, size = 1) +
# Confidence category values
geom_point(aes(color = factor(pid))) +
geom_line(aes(group = factor(pid), color = factor(pid)), alpha = 0.25) +
stat_summary(geom = "errorbar", fun.data = "mean_cl_boot", width = 0.1) +
stat_summary(geom = "point", fun.y = "mean", shape = 23, fill = "black", size = 4) +
# Overall values
geom_violin(data = tmp.2, aes(x="Overall"), fill = "grey", color = NA, alpha = 0.25) +
geom_point(position = position_jitter(w=0.025, h=0),
aes(x="Overall", color = factor(pid)),
data = tmp.2) +
stat_summary(geom = "errorbar", fun.data = "mean_cl_boot", width = 0.1,
aes(x="Overall"), data = tmp.2) +
stat_summary(geom = "point", fun.y = "mean", shape = 23, fill = "black", size = 4,
aes(x="Overall"), data = tmp.2) +
scale_y_continuous(limits = c(0,1), expand = c(0.05,0)) +
scale_x_discrete(expand = c(0,1), labels = c('Low', 'Medium',
'High', 'Overall')) +
scale_color_discrete(name = 'Participant') +
labs(title = "Advisor preference",
legend = NULL,
x = "Confidence",
y = "P(HighAcc chosen)") +
style.long
gg.vi.iii
print('2x2x2 Mixed ANOVA of advisor type x choice x agreement')
aov.vii.i <- aov.influence(trials)
print(summary(aov.vii.i))
print('Means:')
printMeans.influence(trials)
source('F:/www/vhosts/localhost/ExploringSocialMetacognition/analysis/ESM_core.R')
print('2x2x2 Mixed ANOVA of advisor type x choice x agreement')
aov.vii.i <- aov.influence(trials)
print(summary(aov.vii.i))
print('Means:')
printMeans.influence(trials)
tmp <- aggregate(influence ~ adviceType + hasChoice + advisorAgrees + pid, data = trials, FUN = mean)
if(!factorize)
return(tmp)
for(n in names(tmp[-(length(tmp))]))
tmp[ ,n] <- factor(tmp[ ,n])
if(!stripMissing)
return(tmp)
# drop missing observations
miss <- aggregateMissing(tmp, 'pid', list('adviceType', 'hasChoice', 'advisorAgrees'))
tmp <- tmp[!(tmp$pid %in% miss), ]
factorize = T
stripMissing = T
tmp <- aggregate(influence ~ adviceType + hasChoice + advisorAgrees + pid, data = trials, FUN = mean)
if(!factorize)
return(tmp)
for(n in names(tmp[-(length(tmp))]))
tmp[ ,n] <- factor(tmp[ ,n])
if(!stripMissing)
return(tmp)
# drop missing observations
miss <- aggregateMissing(tmp, 'pid', list('adviceType', 'hasChoice', 'advisorAgrees'))
tmp <- tmp[!(tmp$pid %in% miss), ]
unique(tmp$adviceType)
tmp$influence[tmp$adviceType==adviceTypes$aT]
adviceTypes[5]
getAdviceType(aT)
getAdviceType(unique(tmp$adviceType)[1])
getAdviceType(unique(tmp$adviceType))
getAdviceType(tmp$adviceType[1])
tmp$adviceType[1]
getAdviceType(5)
getAdviceTypeById(unique(tmp$adviceType)[1])
getAdviceTypeById(tmp$adviceType[1])
for(aT in unique(tmp$adviceType))
printMean(tmp$influence[tmp$adviceType==aT],
paste0('Mean|', getAdviceTypeName(aT)))
source('F:/www/vhosts/localhost/ExploringSocialMetacognition/analysis/ESM_core.R')
print('2x2x2 Mixed ANOVA of advisor type x choice x agreement')
aov.vii.i <- aov.influence(trials)
print(summary(aov.vii.i))
print('Means:')
printMeans.influence(trials)
gg.vii.ii <- ggplot(tmp, aes(advisorAgrees, influence, color = adviceType, fill = adviceType)) +
geom_point(position = position_jitter(w=0.1, h=0), alpha = 0.5) +
stat_summary(geom = "errorbar",
fun.data = "mean_cl_boot",
width = 0.2) +
stat_summary(geom = "point",
fun.y = "mean",
shape = 23, size = 5) +
stat_summary(aes(group = adviceType), fun.y=mean, geom="line") +
facet_grid(.~hasChoice,
labeller = as_labeller(c('FALSE'='Forced trials','TRUE'='Choice trials'))) +
scale_color_discrete(name = 'Advisor type', labels = getAdviceTypeName(unique(tmp$adviceType))) +
scale_fill_discrete(name = 'Advisor type', labels = getAdviceTypeName(unique(tmp$adviceType))) +
scale_x_discrete(name = 'Judge-advisor agreement', labels = c('Disagree', 'Agree')) +
labs(title = "Advice Influence",
legend = NULL,
y = "Influence") +
style
gg.vii.ii
source('F:/www/vhosts/localhost/ExploringSocialMetacognition/analysis/ESM_core.R')
gg.vii.ii <- gg.influence(trials)
gg.vii.ii
aov.vii.iii <- aov.influence(trials[trials$confidenceCategory==confidenceCategories$medium
& trials$finalAnswer==trials$correctAnswer, ])
print('As above, looking at only trials where intial decision was correct and made with middle confidence:')
print(summary(aov.vii.iii))
print('Means:')
printMeans.influence(trials[trials$confidenceCategory==confidenceCategories$medium
& trials$finalAnswer==trials$correctAnswer, ])
gg.vii.ii <- gg.influence(trials[trials$confidenceCategory==confidenceCategories$medium
& trials$finalAnswer==trials$correctAnswer, ])
gg.vii.ii
gg.vii.ii <- gg.influence(trials[trials$confidenceCategory==confidenceCategories$medium
& trials$finalAnswer==trials$correctAnswer, ]) +
labs(title = 'Advisor Influence on medium-confidence trials')
gg.vii.ii
gg.vii.ii <- gg.influence(trials[trials$confidenceCategory==confidenceCategories$medium
& trials$finalAnswer==trials$correctAnswer, ]) +
labs(title = 'Advisor Influence on medium-confidence trials')
gg.vii.ii
tmp <- trials
tmp$influence <- tmp$rawInfluence
aov.vii.iv <- aov.influence(tmp)
print('Original mixed ANOVA using raw influence scores')
print(summary(aov.vii.iv))
print('Means:')
printMeans.influence(tmp)
#' Print the means for the within-subjects factors
#' @inheritDotParams tmp.influence
#' @inheritParams tmp.influence
#'
printMeans.influence <- function(trials, ...) {
tmp <- tmp.influence(trials, ...)
for(aT in unique(tmp$adviceType))
printMean(tmp$influence[tmp$adviceType==aT],
paste0('Mean|', getAdviceTypeName(aT)))
printMean(tmp$influence[tmp$hasChoice==T], 'Mean|Choice')
printMean(tmp$influence[tmp$hasChoice==F], 'Mean|Forced')
printMean(tmp$influence[tmp$advisorAgrees==T], 'Mean|Agree')
printMean(tmp$influence[tmp$advisorAgrees==F], 'Mean|Disagree')
NULL
}
tmp <- tmp.influence(trials, ...)
for(aT in unique(tmp$adviceType))
printMean(tmp$influence[tmp$adviceType==aT],
paste0('Mean|', getAdviceTypeName(aT)))
printMean(tmp$influence[tmp$hasChoice==T], 'Mean|Choice')
printMean(tmp$influence[tmp$hasChoice==F], 'Mean|Forced')
printMean(tmp$influence[tmp$advisorAgrees==T], 'Mean|Agree')
printMean(tmp$influence[tmp$advisorAgrees==F], 'Mean|Disagree')
NULL
