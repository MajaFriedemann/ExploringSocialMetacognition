---
title: "Marker use analysis"
author: "Matt Jaquiery (matt.jaquiery@psy.ox.ac.uk)"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
    css: ../src/writeUp.css
    includes:
      after_body: ../src/toc_menu.html
  html_notebook:
    toc: yes
    toc_depth: 3
    css: ../src/writeUp.css
editor_options:
  chunk_output_type: inline
---

August 2019

[Script run `r Sys.time()`]

```{r prematter, include = F}

library(binaryLogic)
library(dplyr)
library(prettyMD)


library(testthat)

library(tidyverse)

library(curl)

library(lsr)
library(BayesFactor)
library(BANOVA)
library(ez)

library(knitr)

# opts_chunk$set('echo' = F)

set.seed(20190425)

# Plot setup
theme_set(theme_light() + 
            theme(panel.grid.major.x = element_blank()))

```

## Introduction

Marker use study v0.2.1 is an exact replicaiton of the basic datesStudy, but with slightly different markers.

## Load data

```{r loadData}

studyName <- "markerUse"
studyVersion <- "0-2-1"

exclude <- list(
  maxAttnCheckFails = 0 # pass all attn checks
)

skipLoadData <- F

# source("src/01 Load Data.R")
source("src/02 Exclusions.R")

```

Marker lists used in this study: `r print(getMarkerList())`.

### Exclusions

Exclusions happen in the following order:

* Exclude participants failing attention checks and remove their trials

The numbers excluded for these reasons (participants can be excluded for multiple reasons):

```{r exclusions}

tmp <- suppressWarnings(left_join(exclusions, okayIds, by = "pid"))

tmp$condition <- factor(tmp$condition, labels = c("fb_AgrFirst",
                                                  "fb_AccFirst",
                                                  "¬fb_AgrFirst",
                                                  "¬fb_AccFirst"))

table(tmp$excluded, tmp$condition)

```

# Confirmatory analyses

## Task performance

First we offer a characterisation of the task, to provide the reader with a sense of how the participants performed. 

The statistics for many of these are broken down as a cross-section of two factors, **decision** and **feedback**. **Decision** is a within-subjects variable, and indicates whether the judgement under consideration was the *first* (pre-advice) or *last* (post-advice) decision. **Feedback** is a between-subjects variable, and indicates whether the participant received feedback immediately following the last decision on a trial. Feedback allows participants to track the value of advice directly.

**Note:** *"first" and "last" are used as terms simply because they arrange the factors into alphabetical order with no messing about. Other terms would work equally well (e.g. initial/final is common in the literature).*

### Decisions

Participants offered estimates of the year in which various events took place. The correct answers were always between 1900 and 2000, although the timeline on which participants responded went from 1890 to 2010 in order to allow extra room for advice. Participants answered by dragging a marker onto a timeline. Markers of various widths were available for the participants to choose, with wider markers which covered more years being worth fewer points. Participants were informed that a correct answer was one in which the marker covered the year in which the event took place.

#### Marker usage

##### Table

These markers were used by the participants as described in the table below:

```{r markerUse}
  
tmp <- markerBreakdown(proportion, PP, hideMarkerTotal = T)

# Proportions within a row should sum to 1
for (x in tmp)
  expect_equal(apply(x[, 3:ncol(x)], 1, sum), rep(1, nrow(x)))

num2str.tibble(tmp$first, isProportion = T, precision = 3)
num2str.tibble(tmp$last, isProportion = T, precision = 3)

```

**Marker usage summary table (means) for initial and final decisions**  
*Shows mean marker usage proportion for final and initial decisions for each feedback condition. Columns with NA represent totals across that variable.*  
*Data are aggregated within each participant before combination (and hence do not sum to 1). Except where otherwise mentioned, data presented will be in this manner - aggregations of individual participants' means.*

##### Graph

```{r markerGraph}

ggplot(PP[!is.na(PP$responseMarker), ], 
       aes(x = responseMarker, y = proportion)) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") +
  geom_line(alpha = 2/length(unique(PP$pid)), aes(group = pid), size = 1.25) + 
  geom_point(aes(colour = pid), size = 2) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(feedback ~ decision, labeller = label_both) +
  scale_linetype_manual(values = c("dashed")) + 
  labs(x = "response marker width (years)", 
       y = "p(marker used)")

```

##### Combined graph

It's perhaps more useful to look at this split only by decision.

```{r markerGraphCombined}

ggplot(PP[!is.na(PP$responseMarker), ], 
       aes(x = responseMarker, y = proportion)) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") +
  geom_line(alpha = 2/length(unique(PP$pid)), aes(group = pid), size = 1.25) + 
  geom_point(aes(colour = pid), size = 2) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_wrap( ~ decision, labeller = label_both) +
  scale_linetype_manual(values = c("dashed")) +
  scale_colour_discrete(guide = "none") +
  labs(x = "response marker width (years)", 
       y = "p(marker used)")

```

##### Summary {.summary}

The marker use balance is improving on previous studies. In the first decision, where it is most critical to get an even spread, the latter three markers are seeing a more even distribution. Perhaps adjusting the width of the 10-wide marker down a little will help, although a previous study with marker widths of 7, 14, 21 showed a strong drop-off for the 21-width marker. Perhaps increasing the points for this marker would help.

#### Error (estimate mean)

The **error** is calcualted as the distance from the centre of the answer marker to the correct year. It is thus possible for **correct** answers to have non-zero error, and it is likely that the error for correct answers scales with the marker size.

##### Table

```{r errBlock2}

tmp <- markerBreakdown(responseError, decisions)
num2str.tibble(tmp$first)
num2str.tibble(tmp$last)

```

##### Graph

```{r errGraphBlock2}

ggplot(aggregate(responseError ~ 
                   responseMarker + decision + feedback + pid,
                 decisions, mean), 
       aes(x = responseMarker, y = responseError)) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") +
  geom_boxplot(fill = NA, outlier.color = NA) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(feedback ~ decision, labeller = label_both) +
  scale_linetype_manual(values = c("dashed")) + 
  labs(x = "response marker width (years)", 
       y = "|target - response marker centre| (years)")

```

## Advisor performance

We want to know how the advisors behave. They are programmed to be different, but the actual advice they can offer is limited by the circumstances of a trial. If, for instance, they are instructed to *agree and be correct*, this is only possible if the difference between the edge of the initial response marker and the correct answer is less than the advisor's precision.

### Descriptives

Advice consists in the placement of a marker on the timeline, similar to how participants make their decisions. Advice is classified according to two key properties:  

* **Error** is a measure of accuracy, and is the absolute difference between the correct answer and the middle of the advisor's marker.  

* **Distance** is a measure of agreement, and is the absolute difference between the middle of the participant's initial decision marker and the middle of the advisor's marker.

#### Offbrand trials

Advisors' advice characteristics are presented for the trials as a whole, where differences are expected as a function of advisor (but not feedback or first advisor), and for trials where the advice offered is equivalent between advisors ("**offbrand advice**"). There should not be obvious differences between advisors in terms of their offbrand advice. 

#### Advice offered

The advice offered should be equivalent between feedback conditions.

```{r adviceGiven}

out <- list()
for (f in unique(AdvisedTrial$feedback)) {
  m <- AdvisedTrial$feedback == f
 
  tmp <- NULL
  for (a in advisorNames) {
    r <- tibble(feedback = f, advisor = a)
    for (x in adviceTypes) {
      eq <- as.formula(paste0(a, ".actualType ~ pid"))
      r[, x] <- mean(aggregate(eq, 
                               AdvisedTrial[m, ], 
                               function(q) mean(q == x))[, 2])
    }
    tmp <- rbind(tmp, r)
  }
  
  out[[as.character(f)]] <- tmp
}

prop2str(out$`TRUE`, precision = 3)
prop2str(out$`FALSE`, precision = 3)

```

### Error

Advisors are supposed to differ in their accuracy. These values should be stable between feedback conditions.

```{r adviceAccuracy}

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0(a, ".error ~ pid + feedback + firstAdvisor"))
  r <- aggregate(eq, AdvisedTrial, mean, na.rm = T)
  
  colnames(r) <- c("pid", "feedback", "firstAdvisor", "error")
  r$advisor = a
  
  tmp <- rbind(tmp, as.tibble(r))
}

prop2str(as.tibble(aggregate(error ~ advisor + feedback + firstAdvisor, 
                             tmp, 
                             mean, na.rm = T)), 
         precision = 3)

```

```{r adviceAccuracyGraph}

ggplot(tmp, aes(x = advisor, y = error, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(firstAdvisor ~ feedback, scales = "free_y", labeller = label_both)

```

#### Off-brand advice

Because both advisors use the same off-brand advice, there should be no noticable differences here.

```{r adviceAccuracyOffbrand}

offBrand <- AdvisedTrial[AdvisedTrial$advisor0offBrand == T, ]

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0(a, ".error ~ pid + feedback + firstAdvisor"))
  r <- aggregate(eq, offBrand, mean, na.rm = T)
  
  colnames(r) <- c("pid", "feedback", "firstAdvisor", "error")
  r$advisor = a
  
  tmp <- rbind(tmp, as.tibble(r))
}

prop2str(as.tibble(aggregate(error ~ advisor + feedback + firstAdvisor, 
                             tmp, 
                             mean, na.rm = T)), 
         precision = 3)

```

```{r adviceAccuracyOffbrandGraph}

ggplot(tmp, aes(x = advisor, y = error, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(firstAdvisor ~ feedback, scales = "free_y", labeller = label_both)

```

### Distance

```{r adviceDistance}

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0(a, ".distance ~ pid + feedback + firstAdvisor"))
  r <- aggregate(eq, AdvisedTrial, mean, na.rm = T)
  
  colnames(r) <- c("pid", "feedback", "firstAdvisor", "distance")
  r$advisor = a
  
  tmp <- rbind(tmp, as.tibble(r))
}

prop2str(as.tibble(aggregate(distance ~ advisor + feedback + firstAdvisor, 
                             tmp, 
                             mean, na.rm = T)), 
         precision = 3)

```

```{r adviceDistanceGraph}

ggplot(tmp, aes(x = advisor, y = distance, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(firstAdvisor ~ feedback, labeller = label_both)

```

#### Off-brand advice

There should be no noticeable differences here.

```{r adviceDistanceOffbrand}

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0(a, ".distance ~ pid + feedback + firstAdvisor"))
  r <- aggregate(eq, offBrand, mean, na.rm = T)
  
  colnames(r) <- c("pid", "feedback", "firstAdvisor", "distance")
  r$advisor = a
  
  tmp <- rbind(tmp, as.tibble(r))
}

prop2str(as.tibble(aggregate(distance ~ advisor + feedback + firstAdvisor, 
                             tmp, 
                             mean, na.rm = T)), 
         precision = 3)

```

```{r adviceDistanceOffbrandGraph}

ggplot(tmp, aes(x = advisor, y = distance, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(firstAdvisor ~ feedback, scales = "free_y", labeller = label_both)

```

### Influence

The measure of influence is weight-on-advice. This is our dependent variable in the analysis. This is well-defined for values between 0 and 1 (trucated otherwise), and is
$$\text{WoA} = (\text{final} - \text{inital}) / (\text{advice} - \text{initial})$$
, or the degree to which the final decision moves towards the advised answer. Ill-defined values are truncated to 0 or 1 for analysis.

Influence is the primary outcome measure, and is thus expected to differ between advisors and feedback conditions.

```{r woa}

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0(a, ".woa ~ pid + feedback + firstAdvisor"))
  r <- aggregate(eq, AdvisedTrial, mean, na.rm = T)
  
  colnames(r) <- c("pid", "feedback", "firstAdvisor", "WoA")
  r$advisor <- a
  tmp <- rbind(tmp, r)
}

prop2str(as.tibble(aggregate(WoA ~ advisor + feedback + firstAdvisor, 
                             tmp, 
                             mean, na.rm = T)), 
         precision = 3)

```

```{r woaGraph}

ggplot(tmp, aes(x = advisor, y = WoA, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(firstAdvisor ~ feedback, labeller = label_both)

```

##### Offbrand trials

###### Table

```{r woaOffbrand}
  
tmp <- NULL
for (a in advisorNames) {
  x <- offBrand[offBrand$advisor0idDescription == a, ]
  
  eq <- as.formula(paste0(a, ".woa ~ pid + feedback + firstAdvisor"))
  r <- aggregate(eq, x, mean, na.rm = T)
  
  colnames(r) <- c("pid", "feedback", "firstAdvisor", "WoA")
  r$advisor <- a
  tmp <- rbind(tmp, r)
}

prop2str(as.tibble(aggregate(WoA ~ advisor + feedback + firstAdvisor, 
                             tmp, 
                             mean, na.rm = T)), 
         precision = 3)

```

###### Graph

```{r woaGraphOffbrand}

ggplot(tmp, aes(x = advisor, y = WoA, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(firstAdvisor ~ feedback, labeller = label_both)

```

##### WoA distribution

It's good to keep a general eye on the distribution of weight-on-advice on a trial-by-trial basis. 

```{r woaDistribution}

# update offbrand
offBrand <- AdvisedTrial[AdvisedTrial$advisor0actualType == "disagreeReflected", ]

tmp <- AdvisedTrial[!is.nan(AdvisedTrial$advisor0woaRaw), ]

ggplot(tmp, aes(woa)) + 
  geom_histogram(stat = "count") +
  facet_grid(feedback ~ advisor0idDescription, labeller = label_both)

```

```{r woaExtremes}

AdvisedTrial <- singleAdvisorTrials(AdvisedTrial)

tmp <- 
  AdvisedTrial[AdvisedTrial$woa %in% c("<0", ">=1", ""), 
               c("pid", "advisor0idDescription",  "stimHTML", "number", 
                 "responseEstimateLeft", "responseMarkerWidth", 
                 "responseEstimateLeftFinal", "responseMarkerWidthFinal",
                 "advisor0advice", "advisor0woa", "advisor0woaRaw")]

```

### Participant behaviour following advice

#### Error

Participants should reduce their error as a function of advice, and this is expected to be most pronounced for the Accurate advisors. Here we plot **error reduction**, which (unlike most of the following variables) is obtained with initial - final, as opposed to final - initial. This is because error is expected to be lower on most final decisions than initial decisions, and helpfully makes larger positive values indicative of better performance.

```{r errorReduction}

tmp <- aggregate(errorReduction ~ pid + feedback + firstAdvisor, 
                 AdvisedTrial, mean, na.rm = T)

num2str(as.tibble(aggregate(errorReduction ~ feedback + firstAdvisor, 
                            tmp, mean, na.rm = T)))

```

```{r errorReductionGraph}

ggplot(tmp, aes(x = feedback, y = errorReduction, colour = pid)) + 
  geom_violin(alpha = .25, colour = NA, fill = "grey75") + 
  geom_boxplot(fill = NA, outlier.color = NA, aes(group = feedback)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  geom_point(alpha = .5, aes(colour = pid)) +
  facet_wrap(~firstAdvisor, labeller = label_both)

```

##### Offbrand trials

###### Table

```{r errorReductionOffbrand}

tmp <- aggregate(errorReduction ~ 
                   pid + feedback + firstAdvisor + advisor0idDescription, 
                 offBrand, mean, na.rm = T)

num2str(as.tibble(aggregate(errorReduction ~ 
                              feedback + firstAdvisor + 
                              advisor0idDescription, 
                            tmp, mean, na.rm = T)))

```

##### Graph

```{r errorReductionGraphOffbrand}

ggplot(tmp, 
       aes(x = advisor0idDescription, y = errorReduction, colour = pid)) + 
  geom_hline(yintercept = 0, linetype = "dashed", size = 0.5) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") + 
  geom_boxplot(fill = NA, outlier.color = NA, 
               aes(group = advisor0idDescription)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  geom_point(alpha = .5, aes(colour = pid)) +
  facet_grid(firstAdvisor ~ feedback, labeller = label_both)

```

## Hypothesis testing

The key property to look for in the hypothesis test is an interaction between advisor (*advisor0idDescription*) and feedback (*feedback*). This may appear instead as a three-way advisor x feedback x order (*firstAdvisor*) interaction, but pilot studies indicate the interaction should be two- rather than three-way. 

For now we use the ezANOVA function in the ez package. We'll use a Bayesian version later, but right now it's proving difficult to get working.

Post-hoc tests will break down the results, and they can be visualised from the [weight-on-advice analyses](#Influence) above.

### ANOVA of offbrand trials

```{r ezAnoveOffbrand}

df <- aggregate(advisor0woa ~ 
                   pid + feedback + advisor0idDescription + firstAdvisor,
                 offBrand, mean, na.rm = T)
df$feedback <- factor(df$feedback)

# remove incomplete cases
for (p in unique(df$pid)) {
  if (nrow(df[df$pid == p, ]) != 2) {
    print(paste("Dropping incomplete case pid =", p))
    df <- df[df$pid != p, ]
  }
}

# refactor pid
df$pid <- factor(df$pid)

ezANOVA(df, advisor0woa, pid, 
        within = advisor0idDescription,
        between = list(feedback, firstAdvisor),
        detailed = T,
        type = 2)

```

### Summary {.summary}

As reflected in the WoA graph, it seems participants do treat the advisors differently. Where feedback is provided, the pattern is as expected whereby the accurate advisor is preferred. Where feedback is not provided, it seems participants show a preference for the advice style of the first advisor they encounter. We should note that we don't necessarily have enough participants to draw strong conclusions here, but the suggestion is interesting.

### Post-hoc tests

Advisor preference is quantified as WoA(Accurate) - WoA(Agreeing).

```{r preference}

df <- df[order(df$pid), ]

tmp <- df[df$advisor0idDescription == "Accurate", 
          c("pid", "feedback", "firstAdvisor")]
tmp$AccPref <- df$advisor0woa[df$advisor0idDescription == "Accurate"] - 
  df$advisor0woa[df$advisor0idDescription == "Agreeing"]

```

We want to know preference for accurate over agreeing:

* by feedback (this is the key interaction of interest):

    * This is expected to show that Accuracy preference is stronger in the feedback group

```{r postHocFB, results = 'asis'}

r <- md.ttest(tmp$AccPref[tmp$feedback == T],
              tmp$AccPref[tmp$feedback != T],
              labels = c("*M*|fb", "*M*|¬fb"))

cat(r)

```

* by firstAdvisor:

```{r postHocFA, results = 'asis'}

r <- md.ttest(tmp$AccPref[tmp$firstAdvisor == "Accurate"],
              tmp$AccPref[tmp$firstAdvisor != "Accurate"],
              labels = c("*M*|Acc", "*M*|Agr"))

cat(r)

```

* by feedback and first advisor:

```{r postHocFBFA, results = 'asis'}

# examine by feedback type

r <- md.ttest(tmp$AccPref[tmp$feedback == T & 
                            tmp$firstAdvisor == "Accurate"],
              tmp$AccPref[tmp$feedback == T & 
                            tmp$firstAdvisor != "Accurate"],
              labels = c("*M*|fb,Acc", "*M*|fb,Agr"))

cat(r)

cat("\n\n")

r <- md.ttest(tmp$AccPref[tmp$feedback != T & 
                            tmp$firstAdvisor == "Accurate"],
              tmp$AccPref[tmp$feedback != T & 
                            tmp$firstAdvisor != "Accurate"],
              labels = c("*M*|¬fb,Acc", "*M*|¬fb,Agr"))

cat(r)

```

## Credits 

### Acknowledgements

Thanks as always to Nick Yeung and the other folks at the [ACC Lab](https://www.psy.ox.ac.uk/research/attention-cognitive-control-lab).

### R Packages

```{r results = 'asis'}
# list packages
packageNames <- (.packages())
# don't include very core package
packageNames <- packageNames[!(packageNames %in% 
                                 rownames(installed.packages(
                                   priority = "base")))]
# but do include the base package
packageNames <- c("base", packageNames)
out <- NULL
for (p in packageNames) {
  out <- rbind(out, data.frame('Package' = p, 
                               'Citations' = paste(format(citation(p), 
                                                          style = 'textVersion'), 
                                                   collapse = '<br/><br/>')))
}

kable(out)
```

### Funding

Matt Jaquiery is funded by a studentship from the [Medical Research Council](https://mrc.ukri.org/) (reference 1943590) and the University of Oxford [Department of Experimental Psychology](https://www.psy.ox.ac.uk/) (reference 17/18_MSD_661552).

### Technical details  

```{r results = 'hold'}
cat(paste('Time stamp:', Sys.time(), '\n\n'))
cat('Runtime \n')
proc.time()
cat('\n')
sessionInfo()
```