---
title: "Date estimation analysis"
author: "Matt Jaquiery (matt.jaquiery@psy.ox.ac.uk)"
output:
  html_notebook:
    includes:
      after_body: ../src/toc_menu.html
    toc: yes
    toc_depth: 3
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
editor_options:
  chunk_output_type: inline
---

May 2019

[Script run `r Sys.time()`]

*This is a smaller sample of [coreAnalysis.html](./coreAnalysis.html)*

# Participant exclusions

Many exclusion criteria are only checkable after data have been collected. Our target sample size, as determined by [power analysis](./powerAnalysis.html), is 20 (5 in each of the 4 conditions crossing feedback with first advisor). This is the sample size we need to analyse, so data are collected until there are 5 valid participants in each condition.

Importantly, this means that this script must be run on data as they are collected in order to ascertain where more participants must be recruited. Only the first part of this script (**Participant exclusions**) will be run during this time, and not the analysis part (**Confirmatory analyses**). 

```{r prematter, include = F}

library(binaryLogic)
library(dplyr)
library(prettyMD)


library(testthat)

library(tidyverse)

library(curl)

library(lsr)
library(BayesFactor)
library(BANOVA)
library(ez)

library(knitr)

# opts_chunk$set('echo' = F)

set.seed(20190425)

# Plot setup
theme_set(theme_light() + 
            theme(panel.grid.major.x = element_blank()))

```

## Load data

```{r loadData}

studyVersion <- "1-0-1"

exclude <- data.frame(skipAttnCheck = F, incomplete = T, 
                      outlyingTrials = T, outlyingTrialCount = T,
                      offBrandOutliers = T, minChange = T, 
                      participantOutliers = T, multipleAttempts = T,
                      manual = T, badMarker = T, excess = T)

maxTime <- 60000 # maximum trial time in ms (should pass almost all participants)
maxOutliers <- 3 # maximum outlying trials per participant
minChangePercent <- 0.1 # minimum percent of core trials with different initial and final answers
zThresh <- 3 # threshold for outliers
checkList <- c("timeEnd", "responseError", "responseCorrect") # vars to check for participant outliers
manualExclusion <- #F
  c(F, F, F, F, F, F, F, F, F, T,
    F, F, F, F, F, F, F, T, F, T,
    F, F, F, F, F, F, F, F, T, F,
    F, T, F, F)
nPerCell <- 5

skipLoadData <- F

source("src/02 Exclusions.R")

```

### Exclusions

Exclusions happen in the following order:

* Exclude participants failing attention checks and remove their trials

* Exclude participants who did not complete the study

* Exclude trials with a response time longer than `r maxTime / 1000`s

* Exclude participants with more than `r maxOutliers` trials excluded above and remove their trials

* Exclude participants with any 'offbrand' trials excluded above and remove their trials

* Exclude participants who did not change responsese on at least `r minChangePercent * 100`% of trials

* Exclude participants who were outliers (abs(z) > `r zThresh`) for variables `paste(checkList, collapse = ", ")`

* Exclude participants who previously saw feedback on any of the questions they were offered in the study

* Exclude participants who guessed the within-subjects manipulation

* Exclude participants whose data appears corrupted

* Exclude participants whose data exceeds the required participant count

The numbers excluded for these reasons (participants can be excluded for multiple reasons):

```{r exclusions}

tmp <- suppressWarnings(left_join(exclusions, okayIds, by = "pid"))

tmp$condition <- factor(tmp$condition, labels = c("fb_AgrFirst",
                                                  "fb_AccFirst",
                                                  "¬fb_AgrFirst",
                                                  "¬fb_AccFirst"))

table(tmp$excluded, tmp$condition)

```

# Confirmatory analyses

## Task performance

First we offer a characterisation of the task, to provide the reader with a sense of how the participants performed. 

The statistics for many of these are broken down as a cross-section of two factors, **decision** and **feedback**. **Decision** is a within-subjects variable, and indicates whether the judgement under consideration was the *first* (pre-advice) or *last* (post-advice) decision. **Feedback** is a between-subjects variable, and indicates whether the participant received feedback immediately following the last decision on a trial. Feedback allows participants to track the value of advice directly.

**Note:** *"first" and "last" are used as terms simply because they arrange the factors into alphabetical order with no messing about. Other terms would work equally well (e.g. initial/final is common in the literature).*

### Decisions

Participants offered estimates of the year in which various events took place. The correct answers were always between 1900 and 2000, although the timeline on which participants responded went from 1890 to 2010 in order to allow extra room for advice. Participants answered by dragging a marker onto a timeline. Markers of various widths were available for the participants to choose, with wider markers which covered more years being worth fewer points. Participants were informed that a correct answer was one in which the marker covered the year in which the event took place.

#### Marker usage

Three different markers were available: 

marker | years | points  
-------|------:|-------:
thin   | 1 | 27 | 
medium | 3 | 9 |
wide   | 9 | 3 |

##### Table

These markers were used by the participants as described in the table below:

```{r markerUse}
  
tmp <- markerBreakdown(proportion, PP, hideMarkerTotal = T)

# Proportions within a row should sum to 1
for (x in tmp)
  expect_equal(apply(x[, 3:ncol(x)], 1, sum), rep(1, nrow(x)))

num2str.tibble(tmp$first, isProportion = T, precision = 3)
num2str.tibble(tmp$last, isProportion = T, precision = 3)

```

**Marker usage summary table (means) for initial and final decisions**  
*Shows mean marker usage proportion for final and initial decisions for each feedback condition. Columns with NA represent totals across that variable.*  
*Data are aggregated within each participant before combination (and hence do not sum to 1). Except where otherwise mentioned, data presented will be in this manner - aggregations of individual participants' means.*

##### Graph

```{r markerGraph}

ggplot(PP[!is.na(PP$responseMarker), ], 
       aes(x = responseMarker, y = proportion)) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") +
  geom_boxplot(fill = NA, outlier.color = NA) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(feedback ~ decision, labeller = label_both) +
  scale_linetype_manual(values = c("dashed")) + 
  labs(x = "response marker width (years)", 
       y = "p(marker used)")

```

#### Error (estimate mean)

The **error** is calcualted as the distance from the centre of the answer marker to the correct year. It is thus possible for **correct** answers to have non-zero error, and it is likely that the error for correct answers scales with the marker size.

##### Table

```{r errBlock2}

tmp <- markerBreakdown(responseError, decisions)
num2str.tibble(tmp$first)
num2str.tibble(tmp$last)

```

##### Graph

```{r errGraphBlock2}

ggplot(aggregate(responseError ~ 
                   responseMarker + decision + feedback + pid,
                 decisions, mean), 
       aes(x = responseMarker, y = responseError)) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") +
  geom_boxplot(fill = NA, outlier.color = NA) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(feedback ~ decision, labeller = label_both) +
  scale_linetype_manual(values = c("dashed")) + 
  labs(x = "response marker width (years)", 
       y = "|target - response marker centre| (years)")

```

## Advisor performance

We want to know how the advisors behave. They are programmed to be different, but the actual advice they can offer is limited by the circumstances of a trial. If, for instance, they are instructed to *agree and be correct*, this is only possible if the difference between the edge of the initial response marker and the correct answer is less than the advisor's precision.

### Descriptives

Advice consists in the placement of a marker on the timeline, similar to how participants make their decisions. Advice is classified according to two key properties:  

* **Error** is a measure of accuracy, and is the absolute difference between the correct answer and the middle of the advisor's marker.  

* **Distance** is a measure of agreement, and is the absolute difference between the middle of the participant's initial decision marker and the middle of the advisor's marker.

It is possible for advice to be both accurate and agreeing, to be one but not the other, or to be neither. 

**Advice profiles** determine the kinds of advice an advisor attempts to provide. They specify relative quantities of advice rules, and are selected from a exhaustible pool. In cases where the selected advice rules cannot be fulfilled, a **fallback** rule set is invoked. 

The following advice types are available:

name      | description     | fallback
:--------:|:----------------|:---------------  
**Correctish** | The advisor gives an answer sampled from a normalish distribution around the correct answer | *none*
**Agreeish** | The advisor gives an answer sampled from a normalish distribution around the participant's answer | *none*
**Disagree Reflected** | The advisor gives advice which is the participant's answer reflected in the correct answer, while disagreeing with the participant | Disagree Reversed
**Disagree Reversed** | The advisor gives advice which is the correct answer reflected in the participant's answer, while disagreeing with the participant | *always possible if Disagree Reflected is not*

#### Offbrand trials

Advisors' advice characteristics are presented for the trials as a whole, where differences are expected as a function of advisor (but not feedback or first advisor), and for trials where the advice offered is equivalent between advisors ("**offbrand advice**"). There should not be obvious differences between advisors in terms of their offbrand advice. 

#### Advice offered

The advice offered should be equivalent between feedback conditions.

```{r adviceGiven}

out <- list()
for (f in unique(AdvisedTrial$feedback)) {
  m <- AdvisedTrial$feedback == f
 
  tmp <- NULL
  for (a in advisorNames) {
    r <- tibble(feedback = f, advisor = a)
    for (x in adviceTypes) {
      eq <- as.formula(paste0(a, ".actualType ~ pid"))
      r[, x] <- mean(aggregate(eq, 
                               AdvisedTrial[m, ], 
                               function(q) mean(q == x))[, 2])
    }
    tmp <- rbind(tmp, r)
  }
  
  out[[as.character(f)]] <- tmp
}

prop2str(out$`TRUE`, precision = 3)
prop2str(out$`FALSE`, precision = 3)

```

### Error

Advisors are supposed to differ in their accuracy. These values should be stable between feedback conditions.

```{r adviceAccuracy}

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0(a, ".error ~ pid + feedback + firstAdvisor"))
  r <- aggregate(eq, AdvisedTrial, mean, na.rm = T)
  
  colnames(r) <- c("pid", "feedback", "firstAdvisor", "error")
  r$advisor = a
  
  tmp <- rbind(tmp, as.tibble(r))
}

prop2str(as.tibble(aggregate(error ~ advisor + feedback + firstAdvisor, 
                             tmp, 
                             mean, na.rm = T)), 
         precision = 3)

```

```{r adviceAccuracyGraph}

ggplot(tmp, aes(x = advisor, y = error, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(firstAdvisor ~ feedback, scales = "free_y", labeller = label_both)

```

#### Off-brand advice

Because both advisors use the same off-brand advice, there should be no noticable differences here.

```{r adviceAccuracyOffbrand}

offBrand <- AdvisedTrial[AdvisedTrial$advisor0offBrand == T, ]

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0(a, ".error ~ pid + feedback + firstAdvisor"))
  r <- aggregate(eq, offBrand, mean, na.rm = T)
  
  colnames(r) <- c("pid", "feedback", "firstAdvisor", "error")
  r$advisor = a
  
  tmp <- rbind(tmp, as.tibble(r))
}

prop2str(as.tibble(aggregate(error ~ advisor + feedback + firstAdvisor, 
                             tmp, 
                             mean, na.rm = T)), 
         precision = 3)

```

```{r adviceAccuracyOffbrandGraph}

ggplot(tmp, aes(x = advisor, y = error, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(firstAdvisor ~ feedback, scales = "free_y", labeller = label_both)

```

### Distance

```{r adviceDistance}

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0(a, ".distance ~ pid + feedback + firstAdvisor"))
  r <- aggregate(eq, AdvisedTrial, mean, na.rm = T)
  
  colnames(r) <- c("pid", "feedback", "firstAdvisor", "distance")
  r$advisor = a
  
  tmp <- rbind(tmp, as.tibble(r))
}

prop2str(as.tibble(aggregate(distance ~ advisor + feedback + firstAdvisor, 
                             tmp, 
                             mean, na.rm = T)), 
         precision = 3)

```

```{r adviceDistanceGraph}

ggplot(tmp, aes(x = advisor, y = distance, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(firstAdvisor ~ feedback, labeller = label_both)

```

#### Off-brand advice

There should be no noticeable differences here.

```{r adviceDistanceOffbrand}

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0(a, ".distance ~ pid + feedback + firstAdvisor"))
  r <- aggregate(eq, offBrand, mean, na.rm = T)
  
  colnames(r) <- c("pid", "feedback", "firstAdvisor", "distance")
  r$advisor = a
  
  tmp <- rbind(tmp, as.tibble(r))
}

prop2str(as.tibble(aggregate(distance ~ advisor + feedback + firstAdvisor, 
                             tmp, 
                             mean, na.rm = T)), 
         precision = 3)

```

```{r adviceDistanceOffbrandGraph}

ggplot(tmp, aes(x = advisor, y = distance, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(firstAdvisor ~ feedback, scales = "free_y", labeller = label_both)

```

### Influence

The measure of influence is weight-on-advice. This is our dependent variable in the analysis. This is well-defined for values between 0 and 1 (trucated otherwise), and is
$$\text{WoA} = (\text{final} - \text{inital}) / (\text{advice} - \text{initial})$$
, or the degree to which the final decision moves towards the advised answer. Ill-defined values are truncated to 0 or 1 for analysis.

Influence is the primary outcome measure, and is thus expected to differ between advisors and feedback conditions.

```{r woa}

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0(a, ".woa ~ pid + feedback + firstAdvisor"))
  r <- aggregate(eq, AdvisedTrial, mean, na.rm = T)
  
  colnames(r) <- c("pid", "feedback", "firstAdvisor", "WoA")
  r$advisor <- a
  tmp <- rbind(tmp, r)
}

prop2str(as.tibble(aggregate(WoA ~ advisor + feedback + firstAdvisor, 
                             tmp, 
                             mean, na.rm = T)), 
         precision = 3)

```

```{r woaGraph}

ggplot(tmp, aes(x = advisor, y = WoA, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(firstAdvisor ~ feedback, labeller = label_both)

```

##### Offbrand trials

###### Table

```{r woaOffbrand}
  
tmp <- NULL
for (a in advisorNames) {
  x <- offBrand[offBrand$advisor0idDescription == a, ]
  
  eq <- as.formula(paste0(a, ".woa ~ pid + feedback + firstAdvisor"))
  r <- aggregate(eq, x, mean, na.rm = T)
  
  colnames(r) <- c("pid", "feedback", "firstAdvisor", "WoA")
  r$advisor <- a
  tmp <- rbind(tmp, r)
}

prop2str(as.tibble(aggregate(WoA ~ advisor + feedback + firstAdvisor, 
                             tmp, 
                             mean, na.rm = T)), 
         precision = 3)

```

###### Graph

```{r woaGraphOffbrand}

ggplot(tmp, aes(x = advisor, y = WoA, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(firstAdvisor ~ feedback, labeller = label_both)

```

##### WoA distribution

It's good to keep a general eye on the distribution of weight-on-advice on a trial-by-trial basis. 

```{r woaDistribution}

# update offbrand
offBrand <- AdvisedTrial[AdvisedTrial$advisor0actualType == "disagreeReflected", ]

tmp <- AdvisedTrial[!is.nan(AdvisedTrial$advisor0woaRaw), ]

ggplot(tmp, aes(woa)) + 
  geom_histogram(stat = "count") +
  facet_grid(feedback ~ advisor0idDescription, labeller = label_both)

```

```{r woaExtremes}

AdvisedTrial <- singleAdvisorTrials(AdvisedTrial)

tmp <- 
  AdvisedTrial[AdvisedTrial$woa %in% c("<0", ">=1", ""), 
               c("pid", "advisor0idDescription",  "stimHTML", "number", 
                 "responseEstimateLeft", "responseMarkerWidth", 
                 "responseEstimateLeftFinal", "responseMarkerWidthFinal",
                 "advisor0advice", "advisor0woa", "advisor0woaRaw")]

```

### Participant behaviour following advice

#### Error

Participants should reduce their error as a function of advice, and this is expected to be most pronounced for the Accurate advisors. Here we plot **error reduction**, which (unlike most of the following variables) is obtained with initial - final, as opposed to final - initial. This is because error is expected to be lower on most final decisions than initial decisions, and helpfully makes larger positive values indicative of better performance.

```{r errorReduction}

tmp <- aggregate(errorReduction ~ pid + feedback + firstAdvisor, 
                 AdvisedTrial, mean, na.rm = T)

num2str(as.tibble(aggregate(errorReduction ~ feedback + firstAdvisor, 
                            tmp, mean, na.rm = T)))

```

```{r errorReductionGraph}

ggplot(tmp, aes(x = feedback, y = errorReduction, colour = pid)) + 
  geom_violin(alpha = .25, colour = NA, fill = "grey75") + 
  geom_boxplot(fill = NA, outlier.color = NA, aes(group = feedback)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  geom_point(alpha = .5, aes(colour = pid)) +
  facet_wrap(~firstAdvisor, labeller = label_both)

```

##### Offbrand trials

###### Table

```{r errorReductionOffbrand}

tmp <- aggregate(errorReduction ~ 
                   pid + feedback + firstAdvisor + advisor0idDescription, 
                 offBrand, mean, na.rm = T)

num2str(as.tibble(aggregate(errorReduction ~ 
                              feedback + firstAdvisor + 
                              advisor0idDescription, 
                            tmp, mean, na.rm = T)))

```

##### Graph

```{r errorReductionGraphOffbrand}

ggplot(tmp, 
       aes(x = advisor0idDescription, y = errorReduction, colour = pid)) + 
  geom_hline(yintercept = 0, linetype = "dashed", size = 0.5) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") + 
  geom_boxplot(fill = NA, outlier.color = NA, 
               aes(group = advisor0idDescription)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  geom_point(alpha = .5, aes(colour = pid)) +
  facet_grid(firstAdvisor ~ feedback, labeller = label_both)

```

## Hypothesis testing

The key property to look for in the hypothesis test is an interaction between advisor (*advisor0idDescription*) and feedback (*feedback*). This may appear instead as a three-way advisor x feedback x order (*firstAdvisor*) interaction, but pilot studies indicate the interaction should be two- rather than three-way. 

For now we use the ezANOVA function in the ez package. We'll use a Bayesian version later, but right now it's proving difficult to get working.

Post-hoc tests will break down the results, and they can be visualised from the [weight-on-advice analyses](#Influence) above.

### ANOVA of offbrand trials

```{r ezAnoveOffbrand}

df <- aggregate(advisor0woa ~ 
                   pid + feedback + advisor0idDescription + firstAdvisor,
                 offBrand, mean, na.rm = T)
df$feedback <- factor(df$feedback)

# remove incomplete cases
for (p in unique(df$pid)) {
  if (nrow(df[df$pid == p, ]) != 2) {
    print(paste("Dropping incomplete case pid =", p))
    df <- df[df$pid != p, ]
  }
}

# refactor pid
df$pid <- factor(df$pid)

ezANOVA(df, advisor0woa, pid, 
        within = advisor0idDescription,
        between = list(feedback, firstAdvisor),
        detailed = T,
        type = 2)

```

### Post-hoc tests

Advisor preference is quantified as WoA(Accurate) - WoA(Agreeing).

```{r preference}

df <- df[order(df$pid), ]

tmp <- df[df$advisor0idDescription == "Accurate", 
          c("pid", "feedback", "firstAdvisor")]
tmp$AccPref <- df$advisor0woa[df$advisor0idDescription == "Accurate"] - 
  df$advisor0woa[df$advisor0idDescription == "Agreeing"]

```

We want to know preference for accurate over agreeing:

* by feedback (this is the key interaction of interest):

    * This is expected to show that Accuracy preference is stronger in the feedback group

```{r postHocFB, results = 'asis'}

r <- md.ttest(tmp$AccPref[tmp$feedback == T],
              tmp$AccPref[tmp$feedback != T],
              labels = c("*M*|fb", "*M*|¬fb"))

cat(r)

```

* by firstAdvisor:

```{r postHocFA, results = 'asis'}

r <- md.ttest(tmp$AccPref[tmp$firstAdvisor == "Accurate"],
              tmp$AccPref[tmp$firstAdvisor != "Accurate"],
              labels = c("*M*|Acc", "*M*|Agr"))

cat(r)

```

* by feedback and first advisor:

```{r postHocFBFA, results = 'asis'}

# examine by feedback type

r <- md.ttest(tmp$AccPref[tmp$feedback == T & 
                            tmp$firstAdvisor == "Accurate"],
              tmp$AccPref[tmp$feedback == T & 
                            tmp$firstAdvisor != "Accurate"],
              labels = c("*M*|fb,Acc", "*M*|fb,Agr"))

cat(r)

cat("\n\n")

r <- md.ttest(tmp$AccPref[tmp$feedback != T & 
                            tmp$firstAdvisor == "Accurate"],
              tmp$AccPref[tmp$feedback != T & 
                            tmp$firstAdvisor != "Accurate"],
              labels = c("*M*|¬fb,Acc", "*M*|¬fb,Agr"))

cat(r)

```

## Resiliance testing

We want to ensure the result above is robust to differences in choices made during analysis. A quick way to do this is to look at what happens to the interaction parameter if different exclusion choices are made.

```{r multiverse}

skipLoadData <- F

# Load the raw data and save a copy to speed up the process
source("src/01 Load data.R")

fList <- listServerFiles(studyVersion)
fList <- sub("-", ".", reFirstMatch("([^_]+)\\.csv", fList))

cleanDataFile <- tempfile()

save(fList, file = cleanDataFile)

skipLoadData <- T

fixedExclusions <- exclude[, c("skipAttnCheck", "incomplete", "badMarker",
                               "outlyingTrials")]
exclude <- exclude[, !(names(exclude) %in% names(fixedExclusions))]

multiverse <- NULL

# construct an exhaustive search using binary logic
for (i in seq(2 ^ ncol(exclude))) {
  # binary representation of column number
  tmp <- as.binary(i - 1)
  # pad to length
  if (ncol(exclude) > length(tmp))
    tmp <- c(rep(0, ncol(exclude) - length(tmp)), tmp)
  
  # convert to dataframe
  tmp <- t(tibble(tmp))
  
  multiverse <- rbind(multiverse, tmp)
}

rownames(multiverse) <- NULL
colnames(multiverse) <- names(exclude)

multiverse <- as.tibble(multiverse)

multiverse$incompleteCases <- 0
multiverse$pValue <- 0
multiverse$n <- 0

# progress bar: this will take a while
pb <- txtProgressBar(min = 1, max = nrow(multiverse), style = 3)

for (mva in seq(nrow(multiverse))) {
  setTxtProgressBar(pb, mva)
  
  exclude <- cbind(multiverse[mva, 1:(ncol(multiverse) - 2)], fixedExclusions)
  
  load(cleanDataFile)
  
  source("src/02 Exclusions.R")
  
  offBrand <- AdvisedTrial[AdvisedTrial$advisor0offBrand == T, ]
  
  df <- aggregate(advisor0woa ~ 
                     pid + feedback + advisor0idDescription + firstAdvisor,
                   offBrand, mean, na.rm = T)
  df$feedback <- factor(df$feedback)
  
  x <- 0
  
  # remove incomplete cases
  for (p in unique(df$pid)) {
    if (nrow(df[df$pid == p, ]) != 2) {
      x <- x + 1
      df <- df[df$pid != p, ]
    }
  }
  
  multiverse$incompleteCases[mva] <- x
  
  multiverse$n <- length(unique(df$pid))
  
  # refactor pid
  df$pid <- factor(df$pid)
  
  r <- ezANOVA(df, advisor0woa, pid, 
               within = advisor0idDescription,
               between = list(feedback, firstAdvisor),
               detailed = F,
               type = 2)
  
  multiverse$pValue[mva] <- r$ANOVA$p[r$ANOVA$Effect == 
                                        "feedback:advisor0idDescription"]
}

```

```{r multiversePlot}

multiverse <- multiverse[order(multiverse$pValue), ]
multiverse$x <- 1:nrow(multiverse)

ggplot(multiverse, aes(x, pValue)) +
  geom_hline(yintercept = .05, linetype = "dashed") + 
  geom_point() 

```

## Credits 

### Acknowledgements

Thanks as always to Nick Yeung and the other folks at the [ACC Lab](https://www.psy.ox.ac.uk/research/attention-cognitive-control-lab).

### R Packages

```{r results = 'asis'}
# list packages
packageNames <- (.packages())
# don't include very core package
packageNames <- packageNames[!(packageNames %in% 
                                 rownames(installed.packages(
                                   priority = "base")))]
# but do include the base package
packageNames <- c("base", packageNames)
out <- NULL
for (p in packageNames) {
  out <- rbind(out, data.frame('Package' = p, 
                               'Citations' = paste(format(citation(p), 
                                                          style = 'textVersion'), 
                                                   collapse = '<br/><br/>')))
}

kable(out)
```

### Funding

Matt Jaquiery is funded by a studentship from the [Medical Research Council](https://mrc.ukri.org/) (reference 1943590) and the University of Oxford [Department of Experimental Psychology](https://www.psy.ox.ac.uk/) (reference 17/18_MSD_661552).

### Technical details  

```{r results = 'hold'}
cat(paste('Time stamp:', Sys.time(), '\n\n'))
cat('Runtime \n')
proc.time()
cat('\n')
sessionInfo()
```