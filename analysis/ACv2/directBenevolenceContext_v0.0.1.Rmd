---
title: "Direct benevolence manipulation analysis by context"
author: "Matt Jaquiery (matt.jaquiery@psy.ox.ac.uk)"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
    css: ../src/writeUp.css
    includes:
      after_body: ../src/toc_menu.html
  html_notebook:
    toc: yes
    toc_depth: 3
    css: ../src/writeUp.css
    includes:
      after_body: ../src/toc_menu.html
editor_options:
  chunk_output_type: inline
---

July 2019

[Script run `r Sys.time()`]


```{r prematter, include = F}

library(testthat)

library(tidyverse)
library(broom)

library(BayesFactor)
library(BayesMed)

library(prettyMD)

library(ez)

library(knitr)

opts_chunk$set('echo' = F)

set.seed(20190723)

# Plot setup
theme_set(theme_light() + 
            theme(panel.grid.major.x = element_blank()))

```

```{r loadData, include = F}

studyVersion <- "0-0-1"
studyName <- "directBenevolenceContexts"

overrideMarkerList <- c(11) # Someone managed to get marker width 8 for one question, not sure how

exclude <- list(
  maxAttnCheckFails = 0, # pass all attn checks
  maxTrialRT = 60000,    # trials take < 1 minute
  minTrials = 11,        # at least 11 trials completed
  minChangeRate = .1,    # some advice taken on 10%+ of trials
  qqLableWhitelist = c(  # Advice questionnaire responses must be one of:
    'Deceptive',
    'Possibly Deceptive',
    'Honest'
  ),
  multipleAttempts = T   # exclude multiple attempts
  ) 

skipLoadData <- F

source("src/02_Exclusions.R")

# Re-order factor levels for advice ratings
AdvisedTrial <- AdvisedTrial %>%
  mutate(advisor0questionnaireHonestyLabel = 
           factor(advisor0questionnaireHonestyLabel, 
                  levels = 
                    levels(advisor0questionnaireHonestyLabel)[c(1, 3, 2)]))

```

# Introduction

Observations from [evolutionary models](https://github.com/oxacclab/EvoEgoBias) show that egocentric discounting is a successful strategy in environments where advisors sometimes provide deliberately poor advice. We reason that human participants may show a sensitivity to these contextual factors underlying advice-taking and respond to them in a rational manner. To test the effects of potentially misleading advice, we used a direct benevolence manipulation in a judge-advisor system. Participants had two experimental blocks: in one they were told the advisors would consistently try to help them ("'honest' advisors"), and in the other they were told the advisors may occasionally try to mislead them ("'deceptive' advisors"). In reality, all advisors offered advice drawn from the same distribution, which was centred on the correct answer (i.e. all of the advice was helpful on avereage).

We hypothesised that participants would place greater weight on the advice of the 'honest' vs 'deceptive' advisors, and that this relationship would be mediated by responses to questionnaires asking about the perceived benevolence of the advisors.

The study was [pre-registered](https://osf.io/tu3ev). Differences from the pre-registered script:



# Method

The experimental code is available on [GitHub](https://github.com/oxacclab/ExploringSocialMetacognition), and the experiement can be performed by visiting [https://acclab.psy.ox.ac.uk/~mj221/ESM/ACv2/db.html](https://acclab.psy.ox.ac.uk/~mj221/ESM/ACv2/db.html?PROLIFIC_PID=WriteUp). 

# Results

## Exclusions

```{r exclusions}

tmp <- suppressWarnings(left_join(exclusions, okayIds, by = "pid"))

tmp$condition <- factor(tmp$condition, labels = c("HonestFirst",
                                                  "DeceptiveFirst"))

table(tmp$excluded, tmp$condition)

```

Our final participant list consists of `r length(unique(AdvisedTrial$pid))` participants who completed an average of `r num2str(mean(aggregate(advisor0 ~ pid, AdvisedTrial, length)$advisor0))` trials each.

## Task performance

First we offer a characterisation of the task, to provide the reader with a sense of how the participants performed. 

### Decisions

Participants offered estimates of the year in which various events took place. The correct answers were always between 1900 and 2000, although the timeline on which participants responded went from 1890 to 2010 in order to allow extra room for advice. Participants answered by dragging a marker onto a timeline which covered a range of 11 years (e.g. 1940-1950). Participants were informed that a correct answer was one in which the marker covered the year in which the event took place.

#### Correctness

Responses are regarded as **correct** if the target year is included within the marker range.

```{r accuracy}

tmp <- markerBreakdown(responseCorrect, decisions, hideTotals = T)
tmp <- rbind(tmp$first, tmp$last)[, c(1, 3)]

num2str.tibble(tmp, isProportion = T, precision = 3)

```

```{r accuracyGraph}

ggplot(aggregate(responseCorrect ~ 
                   responseMarker + decision + pid,
                 decisions, mean), 
       aes(x = decision, y = responseCorrect)) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") +
  geom_boxplot(fill = NA, outlier.color = NA) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  scale_linetype_manual(values = c("dashed")) +
  scale_color_discrete(guide = 'none') + 
  labs(x = "decision", 
       y = "p(response correct)")

```

#### Error (estimate mean)

The **error** is calcualted as the distance from the centre of the answer marker to the correct year. It is thus possible for **correct** answers to have non-zero error, and it is likely that the error for correct answers scales with the marker size.

```{r err}

tmp <- markerBreakdown(responseError, decisions, hideTotals = T)
tmp <- rbind(tmp$first, tmp$last)[, c(1, 3)]

num2str.tibble(tmp, isProportion = T, precision = 3)

```

```{r errGraph}

ggplot(aggregate(responseError ~ 
                   responseMarker + decision + pid,
                 decisions, mean), 
       aes(x = decision, y = responseError)) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") +
  geom_boxplot(fill = NA, outlier.color = NA) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  scale_linetype_manual(values = c("dashed")) +
  scale_color_discrete(guide = 'none') + 
  labs(x = "decision", 
       y = "|target - response marker centre| (years)")

```

### Timing

We can look at the response time - the difference between the time the response is opened and the time the response is received.  

```{r time}

decisions$rt <- decisions$responseTimeEstimate - decisions$timeResponseOpen

tmp <- markerBreakdown(rt, decisions, hideTotals = T)
tmp <- rbind(tmp$first, tmp$last)[, c(1, 3)]

num2str.tibble(tmp, isProportion = T, precision = 3)

```

```{r timeGraph}

ggplot(aggregate(rt ~ 
                   responseMarker + decision + pid,
                 decisions, mean), 
       aes(x = decision, 
           y = rt / 1000)) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") +
  geom_boxplot(fill = NA, outlier.color = NA) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  scale_linetype_manual(values = c("dashed")) +
  scale_color_discrete(guide = 'none') + 
  labs(x = "decision", 
       y = "response time (s)")

```

### Summary {.summary}

For the most part, participants' final answers were closer to the correct answer, and more likely to include the correct answer, as compared to their intial answers. This suggests that they took the advice into account. 

*N.B.: participants who do not adjust their answers following advice on >90% of trials are excluded.*

## Advisor performance

The advice offered should be equivalent between in/out group advisors.

### Accuracy

```{r adviceAccuracy}

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0("cbind(", a, ".accurate, ", 
                          a, ".error) ~ pid"))
  r <- aggregate(eq, AdvisedTrial, mean, na.rm = T)
  
  colnames(r) <- c("pid", "accuracy", "error")
  r$advisor = a
  
  tmp <- rbind(tmp, as.tibble(r))
}

prop2str(as.tibble(aggregate(cbind(accuracy, error) ~ advisor, 
                             tmp, 
                             mean, na.rm = T)), 
         precision = 3)

```

```{r adviceAccuracyGraph}

tmp <- gather(tmp, key = "var", value = "value", c("accuracy", "error"))

for (v in unique(tmp$var)) 
  print(
    ggplot(tmp[tmp$var == v, ], aes(x = advisor, y = value, colour = pid)) +
      geom_violin(colour = NA, fill = "grey75", alpha = .25) +
      geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
      geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
      geom_point(alpha = .5, aes(colour = pid)) +
      stat_summary(geom = "line", fun.y = mean,
                   aes(group = 1, linetype = "mean"), size = 1.5) +
      labs(y = v) +
      scale_color_discrete(guide = 'none')
  )

```

### Agreement

```{r adviceAgreement}

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0(a, ".agree ~ pid"))
  r <- aggregate(eq, AdvisedTrial, mean, na.rm = T)
  
  colnames(r) <- c("pid", "agreement")
  r$advisor <- a
  tmp <- rbind(tmp, r)
}

prop2str(as.tibble(aggregate(agreement ~ advisor, 
                             tmp, mean, na.rm = T)), 
         precision = 3)

```

```{r adviceAgreementGraph}

ggplot(tmp, aes(x = advisor, y = agreement, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  scale_color_discrete(guide = 'none')

```

### Distance

Distance is the continuous version of agreement - the difference between the centre of the advice and the centre of the initial estimate. 

```{r adviceDistance}

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0(a, ".distance ~ pid"))
  r <- aggregate(eq, AdvisedTrial, mean, na.rm = T)
  
  colnames(r) <- c("pid", "distance")
  r$advisor = a
  
  tmp <- rbind(tmp, as.tibble(r))
}

prop2str(as.tibble(aggregate(distance ~ advisor, 
                             tmp, 
                             mean, na.rm = T)), 
         precision = 3)

```

```{r adviceDistanceGraph}

ggplot(tmp, aes(x = advisor, y = distance, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  scale_color_discrete(guide = 'none')

```

### Summary {.summary}

The advice offered by the advisors was designed to be equivalent, and was similar in practice. The 'deceptive' advisors were slightly more accurate and had slightly lower average error, although the difference is unlikely to be meaningful. The same is true for agreement and advice distance, although these metrics showed more desirable scores for the 'honest' advisors.

### Influence

The measure of influence is weight-on-advice. This is well-defined for values between 0 and 1 (trucated otherwise), and is
$$\text{WoA} = (\text{final} - \text{inital}) / (\text{advice} - \text{initial})$$
, or the degree to which the final decision moves towards the advised answer.

Influence is the primary outcome measure, and is thus expected to differ between advisors and feedback conditions.

```{r woa}

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0(a, ".woa ~ pid"))
  r <- aggregate(eq, AdvisedTrial, mean, na.rm = T)
  
  colnames(r) <- c("pid", "WoA")
  r$advisor <- a
  tmp <- rbind(tmp, r)
}

prop2str(as.tibble(aggregate(WoA ~ advisor, tmp, mean, na.rm = T)), 
         precision = 3)

```

```{r woaGraph}

ggplot(tmp, aes(x = advisor, y = WoA, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  scale_y_continuous(limits = c(0, 1)) +
  scale_color_discrete(guide = 'none')

```

##### WoA distribution

It's good to keep a general eye on the distribution of weight-on-advice on a trial-by-trial basis. 

```{r woaDistribution}

ggplot(AdvisedTrial, aes(woa)) + 
  geom_histogram(stat = "count") +
  facet_grid(feedback ~ advisor0idDescription, labeller = label_both)

```

```{r percentage of participants with expected pattern}

tmp <- AdvisedTrial %>%
  group_by(pid, advisor0idDescription) %>%
  summarise(woa = mean(advisor0woa)) %>%
  spread(advisor0idDescription, woa) %>%
  dplyr::filter_all(all_vars(!is.na(.))) %>%
  mutate(asExpected = honest > deceptive)

```

### Summary {.summary}

Overall, advice was taken more seriously when it came from 'honest' advisors as opposed to 'deceptive' advisors. This appears to be predominantly driven by an increase in complete discounting of advice from the 'deceptive' advisors. Most (`r round(mean(tmp$asExpected) * 100, 2)`%) participants show a preference for the 'honest' advisors. Formal testing of the hypothesis is below.

#### WoA by Advice

We also want to know if weight on advice differs as a function of how participants rate the advice itself.

```{r woa by advice}

byAdvice <- AdvisedTrial %>%
  group_by(pid, advisor0idDescription, advisor0questionnaireHonestyLabel) %>%
  summarise(woa = mean(advisor0woa),
            n = n()) 

dw <- 1

# Plot ns
AdvisedTrial %>%
  group_by(pid,
           advisor0idDescription,
           advisor0questionnaireHonestyLabel) %>%
  summarise(nTrials = n()) %>%
  # CORRECT for missing (0 count) categories
  spread(advisor0questionnaireHonestyLabel, nTrials) %>%
  mutate_at(vars(-group_cols()), ~ ifelse(is.na(.), 0, .)) %>%
  gather('advisor0questionnaireHonestyLabel', 'nTrials', Deceptive:Honest) %>%
  mutate(advisor0questionnaireHonestyLabel = 
           factor(advisor0questionnaireHonestyLabel)) %>%
  mutate(
    advisor0questionnaireHonestyLabel = 
           factor(advisor0questionnaireHonestyLabel, 
                  levels = 
                    levels(advisor0questionnaireHonestyLabel)[c(1, 3, 2)])) %>%
  # PLOT
  ggplot(aes(x = advisor0questionnaireHonestyLabel, y = nTrials)) +
  geom_point(aes(group = pid),
             position = position_jitter(width = .2, height = 0), 
             alpha = .25) + 
  stat_summary(geom = 'line',
               aes(group = 1),
               fun.y = mean,
               size = 1,
               position = position_dodge(dw)) +
  stat_summary(geom = 'errorbar',
               fun.data = mean_cl_normal,
               width = 0,
               size = 1,
               position = position_dodge(dw)) + 
  guides(colour = 'none') +
  scale_y_continuous(limits = c(0, NA)) +
  facet_wrap(~advisor0idDescription)

# Plot WoA
byAdvice %>% ggplot(aes(x = advisor0questionnaireHonestyLabel,
                        y = woa, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor0questionnaireHonestyLabel)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  scale_y_continuous(limits = c(0, 1)) +
  scale_color_discrete(guide = 'none') +
  facet_wrap(~advisor0idDescription) +
  labs(x = 'Advice Honesty Rating',
       y = 'Weight on Advice')

```

#### Summary {.summary}

As shown by the number of trials labelled a particular way for each advisor group, advice was more likely to be rated as "Honest" if it came from the 'honest' advisors. Both advisor groups show an increase in how influential the advice was as a function of how the advice was rated, and the influence of advice may be slightly higher for the 'honest' advisors at each label level.

## Hypothesis testing

The hypotheses being tested here are:  

1. Participants will place higher weight on the advice of 'honest' advisors, which we will test for advice rated as honest.

2. Participants will place greater weight on advice on honest- vs deceptive-rated advice.

### 1. Group differences

Participants have different weight-on-advice for 'honest' as opposed to 'deceptive' advisors.

```{r h1, results='asis'}

tmp <- AdvisedTrial %>%
  dplyr::filter(advisor0questionnaireHonestyLabel == 'Honest') %>%
  group_by(pid, advisor0idDescription) %>%
  summarise(woa = mean(advisor0woa)) %>%
  spread(advisor0idDescription, woa) %>%
  dplyr::filter_all(all_vars(!is.na(.)))
  

r <- md.ttest(tmp$honest, tmp$deceptive, 
              c("*M*|honest", "*M*|deceptive"), paired = T)
cat(r)

```

### Summary {.summary}

Where advice was labelled as "Honest" by the participants, advice was more influential when coming from 'honest' advisors. Participants are thus less willing to trust advice from a potentially dubious source even when that advice is perceived as being genuine. This effect is moderately sized. 

### 2. Effect of advice rating

Participants should place greater weight on advice they rate as honest vs deceptive. 

```{r h2 woa by advice test, results='asis'}

tmp <- AdvisedTrial %>%
  dplyr::filter(advisor0questionnaireHonestyLabel %in% 
                  c('Honest', 'Deceptive')) %>%
  group_by(
    pid,
    advisor0questionnaireHonestyLabel
  ) %>%
  summarise(woa = mean(advisor0woa, na.rm = T)) %>%
  # strip missing cases
  spread(advisor0questionnaireHonestyLabel, woa) %>%
  dplyr::filter_all(all_vars(!is.na(.)))

r <- md.ttest(tmp$Deceptive, tmp$Honest, 
              c('*M*|deceptive', '*M*|honest'),
              paired = T)
  
cat(r)

```

### Summary {.summary}

Participants are less influenced by advice they perceive as being misleading. This effect is enormous.

## Exploration

### Advisor differences for other advice types

```{r woa by advisor, results='asis'}

for (q in unique(AdvisedTrial$advisor0questionnaireHonestyLabel)) {
  tmp <- AdvisedTrial %>%
    dplyr::filter(advisor0questionnaireHonestyLabel == q) %>%
    group_by(pid, advisor0idDescription) %>%
    summarise(woa = mean(advisor0woa)) 
  
  # unpaired
  r <- md.ttest(tmp$woa[tmp$advisor0idDescription == 'honest'], 
                tmp$woa[tmp$advisor0idDescription == 'deceptive'], 
                c('*M*|honest', '*M*|deceptive'))
  cat(paste0(q, ' (unpaired): ', r, '\n'))
  
  # paired (strip pp with missing cells)
  tmp <- tmp  %>%
    spread(advisor0idDescription, woa) %>% 
    dplyr::filter_all(all_vars(!is.na(.)))
    
  r <- md.ttest(tmp$honest, tmp$deceptive, 
                c("*M*|honest", "*M*|deceptive"), paired = T)
  cat(paste0(q, ' (paired): ', r, '\n\n'))
}


```

### Unpaired advice contrasts by rating

```{r woa by advice tests, results='asis'}

x <- unique(AdvisedTrial$advisor0questionnaireHonestyLabel)

for (contrast in list(x[c(1,2)], x[c(1,3)], x[c(2,3)])) {
  tmp <- AdvisedTrial %>%
    dplyr::filter(advisor0questionnaireHonestyLabel %in% contrast) %>%
    group_by(
      pid,
      advisor0questionnaireHonestyLabel
    ) %>%
    summarise(woa = mean(advisor0woa, na.rm = T)) 
  
  r <- md.ttest(tmp$woa[tmp$advisor0questionnaireHonestyLabel == contrast[1]], 
                tmp$woa[tmp$advisor0questionnaireHonestyLabel == contrast[2]], 
                paste0('*M*|', contrast))
    
  cat(paste0('(unpaired) ', r, '\n'))
  
  tmp <- tmp %>%
    # strip missing cases
    spread(advisor0questionnaireHonestyLabel, woa) %>%
    dplyr::filter_all(all_vars(!is.na(.)))
  
  r <- md.ttest(tmp[[2]], tmp[[3]], 
                paste0('*M*|', contrast),
                paired = T)
    
  cat(paste0('(paired) ', r, '\n\n'))
}


```

### Mean distance of advice by rating and advisor

If participants are more vigilant when receiving advice from the 'deceptive' advisor, they may classify the advice as deceptive more readily. 

```{r advice distance by rating and advisor}

tmp <- AdvisedTrial %>%
  group_by(pid, advisor0idDescription, advisor0questionnaireHonestyLabel) %>%
  summarise(distance = mean(if_else(is.na(honest.distance), 
                                    deceptive.distance, 
                                    honest.distance)))

ggplot(tmp, aes(x = advisor0idDescription, y = distance,
                fill = advisor0questionnaireHonestyLabel)) +
  geom_point(position = position_jitterdodge(jitter.width = .1), alpha = .25) +
  geom_boxplot(alpha = .5, outlier.color = NA)
  

```

#### Summary {.summary}

There does not seem to be a clear difference in the criteria used to judge the deceptiveness of advice as a function of the labels of the advisors.

### WoA where advice is taken

For parity with previous versions we can look at weight on advice for trials where advice is taken: does it still differ between the advisors?

```{r woa for taken advice}

AdvisedTrial %>%
  group_by(pid, advisor0idDescription) %>%
  summarise(adviceIgnored = mean(advisor0woa <= .05)) %>%
  ungroup() %>% group_by(advisor0idDescription) %>%
  summarise(
    p.adviceIgnored_m = mean(adviceIgnored),
    p.adviceIgnored_sd = sd(adviceIgnored)
  )

tmp <- AdvisedTrial %>%
  dplyr::filter(advisor0woa > .05) %>%
  group_by(pid, advisor0idDescription) %>%
  summarise(WoA = mean(advisor0woa)) 

tmp <- tmp %>%
  ungroup() %>%
  group_by(pid, advisor0idDescription) %>%
  summarise(WoA = mean(WoA)) %>%
  spread(advisor0idDescription, WoA) %>% 
  dplyr::filter_all(all_vars(!is.na(.)))
  

r <- md.ttest(tmp$honest, tmp$deceptive, 
              labels = c('honest', 'deceptive'),
              paired = T)

cat(r)

```

```{r woaGraph for taken advice}

tmp %>%
  gather('advisor', 'WoA', -pid) %>%
  ggplot(aes(x = advisor, y = WoA)) +
  geom_violin(colour = NA, aes(fill = advisor), alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor),
               width = .2) +
  geom_line(alpha = .25, aes(group = pid)) + 
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
  labs(x = 'Advisor')

```

# Conclusions {.summary}

Both the properties of the source of advice and the advice itself are important for determining the influence of a piece of advice. That said, the properties of the advice are far more important than the properties of the advisor - people will mostly take advice on its own merit. 

The properties of the advice may have been far more influential because the properties of the advisor influenced the rating of the advice: if it were harder for advice from 'deceptive' advisors to be rated as "Honest" then much of the explanatory power of the advisor would be hidden by the explanatory power of the advice. The exploratory analysis of advice rating distance by advisor indicated that this was not the case, however: participants were more likely to rate advice as "Deceptive" from 'deceptive' advisors, but not more so as a consequence of the distance between their initial estimate and the advice estimate (a measure of advice plausibility). 

It seems, then, that labelling some advisors as potentially 'deceptive' made participants more likely to perceive their advice as "Deceptive". This is a blanket adjustment, and not an interaction with advice distance. Once the advice is labelled by the participant, the labelling of the advisor does make a slight difference in the influence of advice, but this is dwarfed by the difference made by the participant's own labelling of the advice.

# Credits 

## Acknowledgements

Thanks as always to Nick Yeung and the other folks at the [ACC Lab](https://www.psy.ox.ac.uk/research/attention-cognitive-control-lab).

## R Packages

```{r results = 'asis'}
# list packages
packageNames <- (.packages())
# don't include very core package
packageNames <- packageNames[!(packageNames %in% 
                                 rownames(installed.packages(
                                   priority = "base")))]
# but do include the base package
packageNames <- c("base", packageNames)
out <- NULL
for (p in packageNames) {
  out <- rbind(out, data.frame('Package' = p, 
                               'Citations' = paste(format(citation(p), 
                                                          style = 'textVersion'), 
                                                   collapse = '<br/><br/>')))
}

kable(out)
```

## Funding

Matt Jaquiery is funded by a studentship from the [Medical Research Council](https://mrc.ukri.org/) (reference 1943590) and the University of Oxford [Department of Experimental Psychology](https://www.psy.ox.ac.uk/) (reference 17/18_MSD_661552).

## Technical details  

```{r results = 'hold'}
cat(paste('Time stamp:', Sys.time(), '\n\n'))
cat('Runtime \n')
proc.time()
cat('\n')
sessionInfo()
```