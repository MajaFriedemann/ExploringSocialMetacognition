---
title: "Metacognitive advisors"
author: "Matt Jaquiery (matt.jaquiery@psy.ox.ac.uk)"
output:
  html_notebook:
    includes:
      after_body: ../src/toc_menu.html
    toc: yes
    toc_depth: 3
    css: ../src/writeUp.css
  html_document:
    includes:
      after_body: ../src/toc_menu.html
    df_print: paged
    toc: yes
    toc_depth: '3'
    css: ../src/writeUp.css
editor_options:
  chunk_output_type: inline
---

September 2019

[Script run `r Sys.time()`]

```{r prematter, include = F}

library(prettyMD)     # Clean markdown output of tests
library(tidyverse)    # Data wrangling and plotting
library(ez)           # ANOVA

# opts_chunk$set('echo' = F)

set.seed(20190913)

# Plot setup
theme_set(theme_light() + 
            theme(panel.grid.major.x = element_blank()))

```

## Introduction

The marker use studies use the same structure as the original agreement/accuracy experiment. The main difference between the two is the frequency of 'offbrand' trials - they are more frequent in the marker use studies. 

Here we analyse the marker use data, aggregated over all respondents, to determine whether we replicate the results of the original study.

## Load data

```{r loadData}
studyName <- "withConf"
studyVersion <- c("0.0.1")

studyOffBrandTypeNames <- c("agree-offset")

exclude <- list(
  maxAttnCheckFails = 0, # pass all attn checks
  requireComplete = T,   # complete experiment
  maxTrialRT = 60000,    # trials take < 1 minute
  minTrials = 25,        # at least 25 trials completed
  minOffBrandTrials = 6, # no offBrand trials excluded
  minChangeRate = .1,    # some advice taken on 10%+ of trials
  participantOutliers = data.frame(
    varName = c("timeEnd", "responseError", "responseCorrect"),
    zThresh = 3),        # |z-score| for these variables < 3
  multipleAttempts = T,  # exclude multiple attempts
  manual = rep(F, 85)  # exclusions for guessing manipulation
)

exclude$manual <- F # manual exclusions

skipLoadData <- F

# source("src/01 Load Data.R")
source("src/02 Exclusions.R")

```

### Exclusions

Exclusions happen in the following order:

* Exclude participants failing attention checks and remove their trials

* Exclude participants who did not complete the study

* Exclude trials with a response time longer than `r maxTime / 1000`s

* Exclude participants with more than `r maxOutliers` trials excluded above and remove their trials

* Exclude participants with any 'offbrand' trials excluded above and remove their trials

* Exclude participants who did not change responsese on at least `r minChangePercent * 100`% of trials

* Exclude participants who were outliers (abs(z) > `r zThresh`) for variables `paste(checkList, collapse = ", ")`

* Exclude participants who previously saw feedback on any of the questions they were offered in the study

* Exclude participants who guessed the within-subjects manipulation

* Exclude participants whose data appears corrupted

* Exclude participants whose data exceeds the required participant count

The numbers excluded for these reasons (participants can be excluded for multiple reasons):

```{r exclusions}

tmp <- suppressWarnings(left_join(exclusions, okayIds, by = "pid"))

tmp$condition <- factor(tmp$condition, labels = c("fb_AICFirst",
                                                  "fb_AICFirst",
                                                  "¬fb_AICFirst",
                                                  "¬fb_AIUFirst"))

table(tmp$excluded, tmp$condition)

```

The total exclusions are as follows:

```{r totalExclusions}

table(tmp$excluded != F, tmp$condition)

```

# Influence

The measure of influence is weight-on-advice. This is well-defined for values between 0 and 1 (trucated otherwise), and is
$$\text{WoA} = (\text{final} - \text{inital}) / (\text{advice} - \text{initial})$$
, or the degree to which the final decision moves towards the advised answer. 

Influence is the primary outcome measure, and is thus expected to differ between advisors and feedback conditions.

## All trials

### Table

```{r woa}

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0("`", a, ".woa` ~ pid + feedback"))
  r <- aggregate(eq, AdvisedTrialWithConf, mean, na.rm = T)
  
  colnames(r) <- c("pid", "feedback", "WoA")
  r$advisor <- a
  tmp <- rbind(tmp, r)
}

prop2str(as.tibble(aggregate(WoA ~ advisor + feedback, tmp, mean, na.rm = T)), 
         precision = 3)

```

### Graph

```{r woaGraph}

ggplot(tmp, aes(x = advisor, y = WoA, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_wrap(~feedback, labeller = label_both) +
  scale_color_discrete(guide = "none")

```

## Offbrand trials

Offbrand trials is where it really counts because this is where our stats happen. Differences here, where the advice is balanced between advisors, represents differences between _advisors_ rather than differences dependent upon the _advice_ itself.

### Table

```{r woaOffbrand}


offBrand <- 
  AdvisedTrialWithConf[AdvisedTrialWithConf$advisor0actualType == 
                         "agree-offset", ]  

tmp <- NULL
for (a in advisorNames) {
  x <- offBrand[offBrand$advisor0idDescription == a, ]
  
  eq <- as.formula(paste0("`", a, ".woa` ~ pid + feedback"))
  r <- aggregate(eq, x, mean, na.rm = T)
  
  colnames(r) <- c("pid", "feedback", "WoA")
  r$advisor <- a
  tmp <- rbind(tmp, r)
}

prop2str(as.tibble(aggregate(WoA ~ advisor + feedback, tmp, mean, na.rm = T)), 
         precision = 3)

```

### Graph

```{r woaGraphOffbrand}

ggplot(tmp, aes(x = advisor, y = WoA, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_wrap(~feedback, labeller = label_both) +
  scale_color_discrete(guide = "none")

```

## WoA distribution

It's good to keep a general eye on the distribution of weight-on-advice on a trial-by-trial basis (all trials). 

```{r woaDistribution}

ggplot(AdvisedTrialWithConf, aes(woa)) + 
  geom_histogram(stat = "count") +
  facet_grid(feedback ~ advisor0idDescription, labeller = label_both)

```

# ANOVA

Statistical tests of the offbrand trial graph data.

```{r ezAnova}

# calculate woa for the advisor

tmp <- aggregate(advisor0woa ~ 
                   pid + feedback + advisor0idDescription + firstAdvisor,
                 offBrand, mean, na.rm = T)
tmp$feedback <- factor(tmp$feedback)

# remove incomplete cases
for (p in unique(tmp$pid)) {
  if (nrow(tmp[tmp$pid == p, ]) != 2) {
    print(paste("Dropping incomplete case pid =", p))
    tmp <- tmp[tmp$pid != p, ]
  }
}

# refactor pid
tmp$pid <- factor(tmp$pid)

r <- ezANOVA(tmp, advisor0woa, pid, 
             within = advisor0idDescription,
             between = list(feedback, firstAdvisor),
             detailed = T,
             return_aov = T,
             type = 2)

r

```

# Summary {.summary}

The observations for the feedback condition are consistent with the previous results: participants given feedback about their advisors learn to disregard the advice of the agreeing advisor (whose advice includes no new information). The observations for the no-feedback condition are less consistent: while we still see a different pattern from the feedback condition, it appears to a) not be a reversal of the pattern, as seen previously, and b) differ according to which advisor is seen first. This breakdown is shown in the figure below.

where the first advisor is accurate, participants retain a preference for accurate advice over agreeing advice even when no feedback is provided. They likewise retain a preference for agreeing advice where they encounter the agreeing advisor first. This is probably because they are still learning about their own performance in the task: similar effects were found in early versions of the original study. 

Statistically we see a similar pattern to before in that an interaction is observed between feedback and advisor. We also see a three-way interaction between feedback, advisor, and first advisor, as well as main effects of feedback and advisor. 

## Offbrand WoA by Condition

```{r woaOffbrandByCondition}

tmp <- NULL
for (a in advisorNames) {
  x <- offBrand[offBrand$advisor0idDescription == a, ]
  
  eq <- as.formula(paste0("`", a, ".woa` ~ pid + feedback + firstAdvisor"))
  r <- aggregate(eq, x, mean, na.rm = T)
  
  colnames(r) <- c("pid", "feedback", "firstAdvisor", "WoA")
  r$advisor <- a
  tmp <- rbind(tmp, r)
}

ggplot(tmp, aes(x = advisor, y = WoA, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(firstAdvisor~feedback, labeller = label_both) +
  scale_color_discrete(guide = "none")

```

## Credits 

### Acknowledgements

Thanks as always to Nick Yeung and the other folks at the [ACC Lab](https://www.psy.ox.ac.uk/research/attention-cognitive-control-lab).

### R Packages

```{r results = 'asis'}
# list packages
packageNames <- (.packages())
# don't include very core package
packageNames <- packageNames[!(packageNames %in% 
                                 rownames(installed.packages(
                                   priority = "base")))]
# but do include the base package
packageNames <- c("base", packageNames)
out <- NULL
for (p in packageNames) {
  out <- rbind(out, data.frame('Package' = p, 
                               'Citations' = paste(format(citation(p), 
                                                          style = 'textVersion'), 
                                                   collapse = '<br/><br/>')))
}

kable(out)
```

### Funding

Matt Jaquiery is funded by a studentship from the [Medical Research Council](https://mrc.ukri.org/) (reference 1943590) and the University of Oxford [Department of Experimental Psychology](https://www.psy.ox.ac.uk/) (reference 17/18_MSD_661552).

### Technical details  

```{r results = 'hold'}
cat(paste('Time stamp:', Sys.time(), '\n\n'))

cat('Runtime \n')
proc.time()
cat('\n')
sessionInfo()
```