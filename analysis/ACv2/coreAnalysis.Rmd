---
title: "Date estimation analysis"
author: "Matt Jaquiery (matt.jaquiery@psy.ox.ac.uk)"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
  html_notebook:
    includes:
      after_body: ../src/toc_menu.html
    toc: yes
    toc_depth: 3
editor_options:
  chunk_output_type: inline
---

May 2019

[Script run `r Sys.time()`]


```{r prematter, include = F}

library(binaryLogic)
library(dplyr)
library(prettyMD)


library(testthat)

library(tidyverse)

library(curl)

library(lsr)
library(BayesFactor)
library(BANOVA)
library(ez)

library(knitr)

# opts_chunk$set('echo' = F)

set.seed(20190425)

# Plot setup
theme_set(theme_light() + 
            theme(panel.grid.major.x = element_blank()))

```

## Load data

```{r loadData}

studyVersion <- "1-0-1"

exclude <- list(maxAttnCheckFails = 0, # pass all attn checks
                requireComplete = T,   # complete experiment
                maxTrialRT = 60000,    # trials take < 1 minute
                minTrials = 25,        # at least 25 trials completed
                minOffBrandTrials = 6, # no offBrand trials excluded
                minChangeRate = .1,    # some advice taken on 10%+ of trials
                participantOutliers = data.frame(
                  varName = c("timeEnd", "responseError", "responseCorrect"),
                  zThresh = 3),        # |z-score| for these variables < 3
                multipleAttempts = T,  # exclude multiple attempts
                manual = c(F, F, F, F, F, F, F, F, F, T,
                           F, F, F, F, F, F, F, T, F, T,
                           F, F, F, F, F, F, F, F, T, F,
                           F, T, F, F),# exclusions for guessing manipulation
                maxPerCondition = 5)   # exclusion of over-collected data

skipLoadData <- F

source("src/02_Exclusions.R")

```

## Exclusions

```{r exclusions}

tmp <- suppressWarnings(left_join(exclusions, okayIds, by = "pid"))

tmp$condition <- factor(tmp$condition, labels = c("fb_AgrFirst",
                                                  "fb_AccFirst",
                                                  "¬fb_AgrFirst",
                                                  "¬fb_AccFirst"))

table(tmp$excluded, tmp$condition)

```

Our final participant list consists of `r length(unique(PP$pid))` participants who completed an average of `r num2str(mean(aggregate(number ~ pid, PP, function(x) sum(x) / 2)$number))` trials each. `r sum(aggregate(feedback ~ pid, PP, any)$feedback)` of these received feedback on the task, while the remaining `r sum(!aggregate(feedback ~ pid, PP, any)$feedback)` did not. 

## Task performance

```{r bindAdvisors}

# bind feedback property from participants
advisors <- advisors[advisors$pid %in% PP$pid, ]
advisors <- left_join(advisors, unique(PP[c("pid", "feedback")]), "pid")

# drop practice advisors
advisors <- advisors[advisors$idDescription != "Practice", ]

```

```{r analysisPrep}

block2 <- singleAdvisorTrials(AdvisedTrial)
block2Decisions <- singleAdvisorTrials(decisions)

# Also have a subset of trials where off-brand advice is given
offBrand <- block2[block2$advisor0actualType == "disagreeReflected", ]
offBrand <- 
  block2Decisions[block2Decisions$advisor0actualType == "disagreeReflected", ]
```

First we offer a characterisation of the task, to provide the reader with a sense of how the participants performed. 

The statistics for many of these are broken down as a cross-section of two factors, **decision** and **feedback**. **Decision** is a within-subjects variable, and indicates whether the judgement under consideration was the *first* (pre-advice) or *last* (post-advice) decision. **Feedback** is a between-subjects variable, and indicates whether the participant received feedback immediately following the last decision on a trial. Feedback allows participants to track the value of advice directly.

**Note:** *"first" and "last" are used as terms simply because they arrange the factors into alphabetical order with no messing about. Other terms would work equally well (e.g. initial/final is common in the literature).*

### Decisions

Participants offered estimates of the year in which various events took place. The correct answers were always between 1900 and 2000, although the timeline on which participants responded went from 1890 to 2010 in order to allow extra room for advice. Participants answered by dragging a marker onto a timeline. Markers of various widths were available for the participants to choose, with wider markers which covered more years being worth fewer points. Participants were informed that a correct answer was one in which the marker covered the year in which the event took place.

#### Marker usage

Three different markers were available: 

marker | years | points  
-------|------:|-------:
thin   | 3 | 9 | 
medium | 9 | 3 |
wide   | 27| 1 |

##### Table

These markers were used by the participants as described in the table below:

```{r markerUse}
  
tmp <- markerBreakdown(proportion, PP, hideMarkerTotal = T)

# Proportions within a row should sum to 1
for (x in tmp)
  expect_equal(apply(x[, 3:5], 1, sum), rep(1, nrow(x)))

num2str.tibble(tmp$first, isProportion = T, precision = 3)
num2str.tibble(tmp$last, isProportion = T, precision = 3)

```

**Marker usage summary table (means) for initial and final decisions**  
*Shows mean marker usage proportion for final and initial decisions for each feedback condition. Columns with NA represent totals across that variable.*  
*Data are aggregated within each participant before combination (and hence do not sum to 1). Except where otherwise mentioned, data presented will be in this manner - aggregations of individual participants' means.*

##### Graph

```{r markerGraph}

ggplot(PP[!is.na(PP$responseMarker), ], 
       aes(x = responseMarker, y = proportion)) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") +
  geom_boxplot(fill = NA, outlier.color = NA) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(feedback ~ decision, labeller = label_both) +
  scale_linetype_manual(values = c("dashed")) + 
  labs(x = "response marker width (years)", 
       y = "p(marker used)")

```

**Marker usage graph**
*Shows the proportion of marker usage for each participant by decision and feedback status.*

#### Correctness

Responses are regarded as **correct** if the target year is included within the marker range.

```{r accuracy}

tmp <- markerBreakdown(responseCorrect, decisions)
num2str.tibble(tmp$first, isProportion = T, precision = 3)
num2str.tibble(tmp$last, isProportion = T, precision = 3)

```

```{r accuracyGraph}

ggplot(aggregate(responseCorrect ~ 
                   responseMarker + decision + feedback + pid,
                 decisions, mean), 
       aes(x = responseMarker, y = responseCorrect)) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") +
  geom_boxplot(fill = NA, outlier.color = NA) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(feedback ~ decision, labeller = label_both) +
  scale_linetype_manual(values = c("dashed")) + 
  labs(x = "response marker width (years)", 
       y = "p(response correct)")

```

#### Error (estimate mean)

The **error** is calcualted as the distance from the centre of the answer marker to the correct year. It is thus possible for **correct** answers to have non-zero error, and it is likely that the error for correct answers scales with the marker size.

```{r err}

tmp <- markerBreakdown(responseError, decisions)
num2str.tibble(tmp$first)
num2str.tibble(tmp$last)

```

```{r errGraph}

ggplot(aggregate(responseError ~ 
                   responseMarker + decision + feedback + pid,
                 decisions, mean), 
       aes(x = responseMarker, y = responseError)) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") +
  geom_boxplot(fill = NA, outlier.color = NA) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(feedback ~ decision, labeller = label_both) +
  scale_linetype_manual(values = c("dashed")) + 
  labs(x = "response marker width (years)", 
       y = "|target - response marker centre| (years)")

```

##### Single-advisor trials

###### Table

```{r errBlock2}
  
tmp <- markerBreakdown(responseError, block2Decisions)
num2str.tibble(tmp$first)
num2str.tibble(tmp$last)

```

###### Graph

```{r errGraphBlock2}

ggplot(aggregate(responseError ~ 
                   responseMarker + decision + feedback + pid,
                 block2Decisions, mean), 
       aes(x = responseMarker, y = responseError)) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") +
  geom_boxplot(fill = NA, outlier.color = NA) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(feedback ~ decision, labeller = label_both) +
  scale_linetype_manual(values = c("dashed")) + 
  labs(x = "response marker width (years)", 
       y = "|target - response marker centre| (years)")

```

#### Score

Points are scored only on **correct** trials. The points scored for a correct trial are equal to 27/marker width, meaning that the wider a marker is the fewer points are scored. 

```{r score}

tmp <- markerBreakdown(responseScore, decisions)
num2str.tibble(tmp$first)
num2str.tibble(tmp$last)

```

```{r scoreGraph}

ggplot(aggregate(responseScore ~ 
                   responseMarker + decision + feedback + pid,
                 decisions, mean), 
       aes(x = responseMarker, y = responseScore)) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") +
  geom_boxplot(fill = NA, outlier.color = NA) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(feedback ~ decision, labeller = label_both) +
  scale_linetype_manual(values = c("dashed")) + 
  labs(x = "response marker width (years)", 
       y = "points scored")

```

#### Metacognitive performance

We cannot calculate calibration using p(correct), because the marker width serves as the indicator for confidence and changing marker also changes p(correct) by increasing the range of values considered correct. 

We can, however, use one of the following: 

  * error at different marker widths as an indicator of the degree to which increased accuracy is associated with increased precision  
  
  * p(correct|3yr), indicating whether the answer would have been correct had the 3yr marker been used centred in the same place  

### Timing

Firstly, we can consider the time taken for the entire trial.

```{r timeTotal}

tmp <- markerBreakdown(timeEnd, decisions)
num2str.tibble(tmp$last)

```

Second we can look at the response time - the difference between the time the response is opened and the time the response is received.  

```{r timeTotalGraph}

ggplot(aggregate(timeEnd ~ 
                   responseMarker + decision + feedback + pid,
                 decisions, mean), 
       aes(x = responseMarker, 
           y = timeEnd / 1000)) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") +
  geom_boxplot(fill = NA, outlier.color = NA) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(~ feedback, labeller = label_both) +
  scale_linetype_manual(values = c("dashed")) + 
  labs(x = "response marker width (years)", 
       y = "response time (s)")

```

```{r time}

decisions$rt <- decisions$responseTimeEstimate - decisions$timeResponseOpen

tmp <- markerBreakdown(rt, decisions)
num2str.tibble(tmp$first)
num2str.tibble(tmp$last)

```

```{r timeGraph}

ggplot(aggregate(rt ~ 
                   responseMarker + decision + feedback + pid,
                 decisions, mean), 
       aes(x = responseMarker, 
           y = rt / 1000)) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") +
  geom_boxplot(fill = NA, outlier.color = NA) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(feedback ~ decision, labeller = label_both) +
  scale_linetype_manual(values = c("dashed")) + 
  labs(x = "response marker width (years)", 
       y = "response time (s)")

```

## Advisor performance

We want to know how the advisors behave. They are programmed to be different, but the actual advice they can offer is limited by the circumstances of a trial. If, for instance, they are instructed to *agree and be correct*, this is only possible if the difference between the edge of the initial response marker and the correct answer is less than the advisor's precision.

### Descriptives

Advice consists in the placement of a marker on the timeline, similar to how participants make their decisions. Advice is classified according to two key properties:  

* **Agreement** occurs when there is at least one year of overlap between the participant's marker and the advisor's marker.  

* **Accuracy**. Advice is said to be accurate when the advisor's marker touches the target year.

It is possible for advice to be both accurate and agreeing, to be one but not the other, or to be neither. Each of these cases are expected to occur within the study for most participants, according to most advice profiles.

**Advice profiles** determine the kinds of advice an advisor attempts to provide. They specify relative quantities of advice rules, and are selected from a exhaustible pool. In cases where the selected advice rules cannot be fulfilled (e.g.*agree and be correct* where the participant's answer is too far from the correct answer), a **fallback** rule set is invoked. 

The following advice types are available:

name      | description     | fallback
:--------:|:----------------|:---------------  
**Correct** | The advisor gives a correct answer | *none*
**Correctish** | The advisor gives an answer sampled from a normalish distribution around the correct answer | *none*
**Agree** | The advisor gives an agreeing answer | *none*
**Agreeish** | The advisor gives an answer sampled from a normalish distribution around the participant's answer | *none*
**Correct Agree** | The advisor gives advice which is both correct and agreeing | Agree
**Correct Disagree** | The advisor gives advice which is correct, but which does not agree | Correct
**Disagree Reflected** | The advisor gives advice which is the participant's answer reflected in the correct answer, while disagreeing with the participant | Disagree Reversed
**Disagree Reversed** | The advisor gives advice which is the correct answer reflected in the participant's answer, while disagreeing with the participant | *always possible if Disagree Reflected is not*

#### Advice profile order

The advisors appear in different orders across blocks. The first advisor (position 0) is introduced first, appears above the second, and on gives its advice first on each trial. The order is randomised each block, and thus on average should balance out to around 0.5 for each advisor. 

```{r adviceOrder} 

if (nrow(block2) != nrow(AdvisedTrial))
  as.tibble(aggregate(meanPosition ~ idDescription + feedback, advisors, mean))

```

#### Advice offered

The advice offered, both nominal and actual, should be equivalent between feedback conditions.

The **nominal type** of the advice is the advice selected for the advisor to give.
```{r adviceRequested}

out <- list()
for (f in unique(AdvisedTrial$feedback)) {
  m <- AdvisedTrial$feedback == f
    
  tmp <- NULL
  for (a in advisorNames) {
    r <- tibble(feedback = f, advisor = a)
    for (x in adviceTypes) {
      eq <- as.formula(paste0(a, ".nominalType ~ pid"))
      r[, x] <- mean(aggregate(eq, 
                               AdvisedTrial[m, ], 
                               function(q) mean(q == x))[, 2])
    }
    tmp <- rbind(tmp, r)
  }

  out[[as.character(f)]] <- tmp
}

prop2str(out$`TRUE`, precision = 3)
prop2str(out$`FALSE`, precision = 3)

```

The **actual type** of advice is the advice the advisor actually gave, i.e. allowing for fallbacks where the requested advice type could not be supplied. 

```{r adviceGiven}

out <- list()
for (f in unique(AdvisedTrial$feedback)) {
  m <- AdvisedTrial$feedback == f
 
  tmp <- NULL
  for (a in advisorNames) {
    r <- tibble(feedback = f, advisor = a)
    for (x in adviceTypes) {
      eq <- as.formula(paste0(a, ".actualType ~ pid"))
      r[, x] <- mean(aggregate(eq, 
                               AdvisedTrial[m, ], 
                               function(q) mean(q == x))[, 2])
    }
    tmp <- rbind(tmp, r)
  }
  
  out[[as.character(f)]] <- tmp
}

prop2str(out$`TRUE`, precision = 3)
prop2str(out$`FALSE`, precision = 3)

```

### Accuracy

Advisors are supposed to differ in their accuracy. This is expected to hold true whether accuracy is cast in terms of the proportion of advice which is correct or mean error. These values should also be stable between feedback conditions.

```{r adviceAccuracy}

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0("cbind(", a, ".accurate, ", 
                          a, ".error) ~ pid + feedback"))
  r <- aggregate(eq, AdvisedTrial, mean, na.rm = T)
  
  colnames(r) <- c("pid", "feedback", "accuracy", "error")
  r$advisor = a
  
  tmp <- rbind(tmp, as.tibble(r))
}

prop2str(as.tibble(aggregate(cbind(accuracy, error) ~ advisor + feedback, 
                             tmp, 
                             mean, na.rm = T)), 
         precision = 3)

```

```{r adviceAccuracyGraph}

tmp <- gather(tmp, "var", "value", accuracy:error)

ggplot(tmp, aes(x = advisor, y = value, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(var ~ feedback, scales = "free_y", labeller = label_both)

```

#### Off-brand advice

Off-brand advice occurs when the advisor offers its minority advice type (i.e. disagreeReflected). This allows comparison of perception of advisors (based on their advice on previous trials) while controlling for differences in their actual advice (on the current trial).

Because both advisors use the same off-brand advice, there should be no noticable differences here.

```{r adviceAccuracyOffbrand}

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0("cbind(", a, ".accurate, ", 
                          a, ".error) ~ pid + feedback"))
  r <- aggregate(eq, offBrand, mean, na.rm = T)
  
  colnames(r) <- c("pid", "feedback", "accuracy", "error")
  r$advisor = a
  
  tmp <- rbind(tmp, as.tibble(r))
}

prop2str(as.tibble(aggregate(cbind(accuracy, error) ~ advisor + feedback, 
                             tmp, 
                             mean, na.rm = T)), 
         precision = 3)

```

```{r adviceAccuracyOffbrandGraph}

tmp <- gather(tmp, "var", "value", accuracy:error)

ggplot(tmp, aes(x = advisor, y = value, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_grid(var ~ feedback, scales = "free_y", labeller = label_both)

```

### Agreement

Advisor are also supposed to differ in their agreement rates.

```{r adviceAgreement}

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0(a, ".agree ~ pid + feedback"))
  r <- aggregate(eq, AdvisedTrial, mean, na.rm = T)
  
  colnames(r) <- c("pid", "feedback", "agreement")
  r$advisor <- a
  tmp <- rbind(tmp, r)
}

prop2str(as.tibble(aggregate(agreement ~ advisor + feedback, 
                             tmp, mean, na.rm = T)), 
         precision = 3)

```

```{r adviceAgreementGraph}

ggplot(tmp, aes(x = advisor, y = agreement, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_wrap(~feedback, labeller = label_both)

```

#### Off-brand advice

There should be no noticeable differences here.

```{r adviceAgreementOffbrand}

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0(a, ".agree ~ pid + feedback"))
  r <- aggregate(eq, offBrand, mean, na.rm = T)
  
  colnames(r) <- c("pid", "feedback", "agreement")
  r$advisor = a
  
  tmp <- rbind(tmp, as.tibble(r))
}

prop2str(as.tibble(aggregate(agreement ~ advisor + feedback, 
                             tmp, 
                             mean, na.rm = T)), 
         precision = 3)

```

```{r adviceAgreementOffbrandGraph}

ggplot(tmp, aes(x = advisor, y = agreement, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_wrap(~ feedback, scales = "free_y", labeller = label_both)

```

### Distance

Distance is the continuous version of agreement - the difference between the centre of the advice and the centre of the initial estimate. 

```{r adviceDistance}

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0(a, ".distance ~ pid + feedback"))
  r <- aggregate(eq, AdvisedTrial, mean, na.rm = T)
  
  colnames(r) <- c("pid", "feedback", "distance")
  r$advisor = a
  
  tmp <- rbind(tmp, as.tibble(r))
}

prop2str(as.tibble(aggregate(distance ~ advisor + feedback, 
                             tmp, 
                             mean, na.rm = T)), 
         precision = 3)

```

```{r adviceDistanceGraph}

ggplot(tmp, aes(x = advisor, y = distance, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_wrap(~ feedback, labeller = label_both)

```

#### Off-brand advice

There should be no noticeable differences here.

```{r adviceDistanceOffbrand}

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0(a, ".distance ~ pid + feedback"))
  r <- aggregate(eq, offBrand, mean, na.rm = T)
  
  colnames(r) <- c("pid", "feedback", "distance")
  r$advisor = a
  
  tmp <- rbind(tmp, as.tibble(r))
}

prop2str(as.tibble(aggregate(distance ~ advisor + feedback, 
                             tmp, 
                             mean, na.rm = T)), 
         precision = 3)

```

```{r adviceDistanceOffbrandGraph}

ggplot(tmp, aes(x = advisor, y = distance, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_wrap(~ feedback, scales = "free_y", labeller = label_both)

```

### Influence

The measure of influence is weight-on-advice. This is well-defined for values between 0 and 1 (trucated otherwise), and is
$$\text{WoA} = (\text{final} - \text{inital}) / (\text{advice} - \text{initial})$$
, or the degree to which the final decision moves towards the advised answer.

Influence is the primary outcome measure, and is thus expected to differ between advisors and feedback conditions.

```{r woa}

tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0(a, ".woa ~ pid + feedback"))
  r <- aggregate(eq, AdvisedTrial, mean, na.rm = T)
  
  colnames(r) <- c("pid", "feedback", "WoA")
  r$advisor <- a
  tmp <- rbind(tmp, r)
}

prop2str(as.tibble(aggregate(WoA ~ advisor + feedback, tmp, mean, na.rm = T)), 
         precision = 3)

```

```{r woaGraph}

ggplot(tmp, aes(x = advisor, y = WoA, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_wrap(~feedback, labeller = label_both)

```

##### Single-advisor trials

###### Table

```{r woaBlock2}
  
tmp <- NULL
for (a in advisorNames) {
  x <- block2[block2$advisor0idDescription == a, ]
  
  eq <- as.formula(paste0(a, ".woa ~ pid + feedback"))
  r <- aggregate(eq, x, mean, na.rm = T)
  
  colnames(r) <- c("pid", "feedback", "WoA")
  r$advisor <- a
  tmp <- rbind(tmp, r)
}

prop2str(as.tibble(aggregate(WoA ~ advisor + feedback, tmp, mean, na.rm = T)), 
         precision = 3)

```

###### Graph

```{r woaGraphBlock2}

ggplot(tmp, aes(x = advisor, y = WoA, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_wrap(~feedback, labeller = label_both)

```


##### Offbrand trials

###### Table

```{r woaOffbrand}
  
tmp <- NULL
for (a in advisorNames) {
  x <- offBrand[offBrand$advisor0idDescription == a, ]
  
  eq <- as.formula(paste0(a, ".woa ~ pid + feedback"))
  r <- aggregate(eq, x, mean, na.rm = T)
  
  colnames(r) <- c("pid", "feedback", "WoA")
  r$advisor <- a
  tmp <- rbind(tmp, r)
}

prop2str(as.tibble(aggregate(WoA ~ advisor + feedback, tmp, mean, na.rm = T)), 
         precision = 3)

```

###### Graph

```{r woaGraphOffbrand}

ggplot(tmp, aes(x = advisor, y = WoA, colour = pid)) +
  geom_violin(colour = NA, fill = "grey75", alpha = .25) +
  geom_boxplot(outlier.colour = NA, fill = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_wrap(~feedback, labeller = label_both)

```

##### WoA distribution

It's good to keep a general eye on the distribution of weight-on-advice on a trial-by-trial basis. 

```{r woaDistribution}

low <- 0
high <- 1
n <- 11

block2$woa <- ""
for (x in c("woa", "woaRaw")) {
  block2[, paste0("advisor0", x)] <- sapply(1:nrow(block2), function(i)
    unlist(block2[i, 
                  paste0(as.character(block2$advisor0idDescription[i]), 
                         ".", x)]))
}

block2$woa[block2$advisor0woaRaw >= 1] <- ">=1"
for (x in rev(seq(low, high, length.out = n))) {
  block2$woa[block2$advisor0woaRaw < x] <- paste0("<", x)
}
block2$woa <- factor(block2$woa)

# update offbrand
offBrand <- block2[block2$advisor0actualType == "disagreeReflected", ]

tmp <- block2[!is.nan(block2$advisor0woaRaw), ]

ggplot(tmp, aes(woa)) + 
  geom_histogram(stat = "count") +
  facet_grid(feedback ~ advisor0idDescription, labeller = label_both)

```

```{r woaExtremes}

tmp <- 
  block2[block2$woa %in% c("<0", ">=1", ""), c("pid", "advisor0idDescription", 
                                               "stimHTML", "number", 
                                               "responseEstimateLeft",
                                               "responseMarkerWidth",
                                               "responseEstimateLeftFinal",
                                               "responseMarkerWidthFinal",
                                               "advisor0advice",
                                               "advisor0woa", 
                                               "advisor0woaRaw")]

```

#### Error

Participants should reduce their error as a function of advice, and this is expected to be most pronounced for the Accurate advisors. Here we plot **error reduction**, which (unlike most of the following variables) is obtained with initial - final, as opposed to final - initial. This is because error is expected to be lower on most final decisions than initial decisions, and helpfully makes larger positive values indicative of better performance.

```{r errorReduction}

tmp <- aggregate(errorReduction ~ pid + feedback, 
                 AdvisedTrial, mean, na.rm = T)

num2str(as.tibble(aggregate(errorReduction ~ feedback, 
                            tmp, mean, na.rm = T)))

```

```{r errorReductionGraph}

ggplot(tmp, aes(x = feedback, y = errorReduction, colour = pid)) + 
  geom_violin(alpha = .25, colour = NA, fill = "grey75") + 
  geom_boxplot(fill = NA, outlier.color = NA, aes(group = feedback)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  geom_point(alpha = .5, aes(colour = pid))

```

##### Single-advisor trials

###### Table

```{r errorReductionBlock2}

tmp <- aggregate(errorReduction ~ pid + feedback + advisor0idDescription, 
                 block2, mean, na.rm = T)

num2str(as.tibble(aggregate(errorReduction ~ 
                              feedback + advisor0idDescription, 
                            tmp, mean, na.rm = T)))

```

##### Graph

```{r errorReductionGraphBlock2}

ggplot(tmp, 
       aes(x = advisor0idDescription, y = errorReduction, colour = pid)) + 
  geom_hline(yintercept = 0, linetype = "dashed", size = 0.5) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") + 
  geom_boxplot(fill = NA, outlier.color = NA, 
               aes(group = advisor0idDescription)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  geom_point(alpha = .5, aes(colour = pid)) +
  facet_wrap(~feedback, labeller = label_both)

```

##### Offbrand trials

###### Table

```{r errorReductionOffbrand}

tmp <- aggregate(errorReduction ~ pid + feedback + advisor0idDescription, 
                 offBrand, mean, na.rm = T)

num2str(as.tibble(aggregate(errorReduction ~ 
                              feedback + advisor0idDescription, 
                            tmp, mean, na.rm = T)))

```

##### Graph

```{r errorReductionGraphOffbrand}

ggplot(tmp, 
       aes(x = advisor0idDescription, y = errorReduction, colour = pid)) + 
  geom_hline(yintercept = 0, linetype = "dashed", size = 0.5) +
  geom_violin(alpha = .25, colour = NA, fill = "grey75") + 
  geom_boxplot(fill = NA, outlier.color = NA, 
               aes(group = advisor0idDescription)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  geom_point(alpha = .5, aes(colour = pid)) +
  facet_wrap(~feedback, labeller = label_both)

```

#### Accuracy changes

Participants may benefit from advice in terms of the accuracy of their responses. Here we code a correct response ammended to an incorrect response as -1, responses whose correctness is unchanged as 0, and incorrect responses ammended to correct responses as 1. 

```{r accuracyChanges}

tmp <- aggregate(accuracyChange ~ pid + feedback, 
                 AdvisedTrial, mean, na.rm = T)

num2str(as.tibble(aggregate(accuracyChange ~ feedback, 
                            tmp, mean, na.rm = T)))

```

```{r accuracyChangeGraph}

ggplot(tmp, aes(x = feedback, y = accuracyChange, colour = pid)) + 
  geom_violin(alpha = .25, colour = NA, fill = "grey75") + 
  geom_boxplot(fill = NA, outlier.color = NA, aes(group = feedback)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  geom_point(alpha = .5, aes(colour = pid))

```

##### Single-advisor trials

###### Table

```{r accuracyChangesBlock2}

tmp <- aggregate(accuracyChange ~ pid + feedback + advisor0idDescription, 
                 block2, mean, na.rm = T)

num2str(as.tibble(aggregate(accuracyChange ~ 
                              feedback + advisor0idDescription, 
                            tmp, mean, na.rm = T)))

```

##### Graph

```{r accuracyChangeGraphBlock2}

ggplot(tmp, 
       aes(x = advisor0idDescription, y = accuracyChange, colour = pid)) + 
  geom_violin(alpha = .25, colour = NA, fill = "grey75") + 
  geom_boxplot(fill = NA, outlier.color = NA, 
               aes(group = advisor0idDescription)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  geom_point(alpha = .5, aes(colour = pid)) +
  facet_wrap(~feedback, labeller = label_both)

```

#### Score changes

Score changes are very straightforward - simply the points awarded for the final response minus the points awarded for the initial response, so positive values indicate improvements in score while negative values indicate the final response was less valuable than the initial response.

```{r scoreChange}

tmp <- aggregate(scoreChange ~ pid + feedback, AdvisedTrial, mean, na.rm = T)

num2str(as.tibble(aggregate(scoreChange ~ feedback, tmp, mean, na.rm = T)))

```

```{r scoreChangeGraph}

ggplot(tmp, aes(x = feedback, y = scoreChange, colour = pid)) + 
  geom_violin(alpha = .25, colour = NA, fill = "grey75") + 
  geom_boxplot(fill = NA, outlier.color = NA, aes(group = feedback)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  geom_point(alpha = .5, aes(colour = pid))

```

##### Single-advisor trials

###### Table

```{r scoreChangesBlock2}

tmp <- aggregate(scoreChange ~ pid + feedback + advisor0idDescription, 
                 block2, mean, na.rm = T)

num2str(as.tibble(aggregate(scoreChange ~ 
                              feedback + advisor0idDescription, 
                            tmp, mean, na.rm = T)))

```

##### Graph

```{r scoreChangeGraphBlock2}

ggplot(tmp, aes(x = advisor0idDescription, y = scoreChange, colour = pid)) + 
  geom_violin(alpha = .25, colour = NA, fill = "grey75") + 
  geom_boxplot(fill = NA, outlier.color = NA, 
               aes(group = advisor0idDescription)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  geom_point(alpha = .5, aes(colour = pid)) +
  facet_wrap(~feedback, labeller = label_both)

```

#### Mean shifts

It can be helpful to get a sense of how much participants are adjusting their responses in general, i.e. the mean distance between their first and last responses. 

```{r meanShift}

tmp <- aggregate(estimateLeftChange ~ pid + feedback, 
                 AdvisedTrial, mean, na.rm = T)

num2str(as.tibble(aggregate(estimateLeftChange ~ feedback, 
                            tmp, mean, na.rm = T)))

```

```{r meanShiftGraph}

ggplot(tmp, aes(x = feedback, y = estimateLeftChange, colour = pid)) + 
  geom_violin(alpha = .25, colour = NA, fill = "grey75") + 
  geom_boxplot(fill = NA, outlier.color = NA, aes(group = feedback)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) + 
  geom_point(alpha = .5, aes(colour = pid))

```

#### Single advisor trials

##### Table

```{r meanShiftBlock2}

tmp <- aggregate(estimateLeftChange ~ 
                   pid + feedback + advisor0idDescription, 
                 block2, mean, na.rm = T)

num2str(as.tibble(aggregate(estimateLeftChange ~ 
                              feedback + advisor0idDescription, 
                            tmp, mean, na.rm = T)))

```

###### Graph

```{r meanShiftGraphBlock2}

ggplot(tmp, 
       aes(x = advisor0idDescription, y = estimateLeftChange, colour = pid)) + 
  geom_violin(alpha = .25, colour = NA, fill = "grey75") + 
  geom_boxplot(fill = NA, outlier.color = NA, 
               aes(group = advisor0idDescription)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  facet_wrap(~feedback, labeller = label_both)

```

#### Offbrand trials

##### Table

```{r meanShiftOffbrand}

tmp <- aggregate(estimateLeftChange ~ 
                   pid + feedback + advisor0idDescription, 
                 offBrand, mean, na.rm = T)

num2str(as.tibble(aggregate(estimateLeftChange ~ 
                              feedback + advisor0idDescription, 
                            tmp, mean, na.rm = T)))

```

###### Graph

```{r meanShiftGraphOffbrand}

ggplot(tmp, 
       aes(x = advisor0idDescription, y = estimateLeftChange, colour = pid)) + 
  geom_violin(alpha = .25, colour = NA, fill = "grey75") + 
  geom_boxplot(fill = NA, outlier.color = NA, 
               aes(group = advisor0idDescription)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  facet_wrap(~feedback, labeller = label_both)

```

#### Confidence changes

Marker widths are coded as 3, 2, and 1 for 3, 9, and 27 years respectively. Final - initial confidence gives the change. 

```{r confidenceChange}

tmp <- aggregate(confidenceChange ~ pid + feedback, 
                 AdvisedTrial, mean, na.rm = T)

num2str(as.tibble(aggregate(confidenceChange ~ feedback, 
                            tmp, mean, na.rm = T)))

```

```{r confidenceChangeGraph}

ggplot(tmp, aes(x = feedback, y = confidenceChange, colour = pid)) + 
  geom_violin(alpha = .25, colour = NA, fill = "grey75") + 
  geom_boxplot(fill = NA, outlier.color = NA, aes(group = feedback)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  geom_point(alpha = .5, aes(colour = pid))

```

#### Single advisor trials

##### Table

```{r confidenceShiftBlock2}

tmp <- aggregate(confidenceChange ~ 
                   pid + feedback + advisor0idDescription, 
                 block2, mean, na.rm = T)

num2str(as.tibble(aggregate(confidenceChange ~ 
                              feedback + advisor0idDescription, 
                            tmp, mean, na.rm = T)))

```

###### Graph

```{r confidenceShiftGraphBlock2}

ggplot(tmp, 
       aes(x = advisor0idDescription, y = confidenceChange, colour = pid)) + 
  geom_violin(alpha = .25, colour = NA, fill = "grey75") + 
  geom_boxplot(fill = NA, outlier.color = NA, 
               aes(group = advisor0idDescription)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  facet_wrap(~feedback, labeller = label_both)

```

#### Agreement changes

Advisor agreement is calculated using the participant's initial decision and the advice. In the same way, it is possible to calculate agreement between the advice and the participant's final decision, and to examine whether this changes (i.e. whether the participant shifts to follow disagreeing advice).

```{r agreementChange}

eq <- NULL
tmp <- NULL
for (a in advisorNames) {
  eq <- as.formula(paste0(a, ".agreementChange", " ~ pid + feedback"))
  
  x <- aggregate(eq, AdvisedTrial, mean, na.rm = T)
  
  if (is.null(tmp)) {
    tmp <- x
  } else {
    tmp <- left_join(tmp, x, by = c("pid", "feedback"))
  }
}

tmp <- gather(tmp, "advisor", "agreementChange", 
              grep("\\.agreementChange", names(tmp)))
tmp$advisor <- sapply(tmp$advisor, function(s) reFirstMatch("([^\\.]+)", s))

num2str(as.tibble(aggregate(agreementChange ~ feedback, 
                            tmp, mean, na.rm = T)))

```

```{r agreementChangeGraph}

ggplot(tmp, aes(x = advisor, y = agreementChange, colour = pid)) + 
  geom_violin(alpha = .25, colour = NA, fill = "grey75") + 
  geom_boxplot(fill = NA, outlier.color = NA, aes(group = advisor)) +
  geom_line(alpha = .5, aes(colour = pid, group = pid)) + 
  geom_point(alpha = .5, aes(colour = pid)) +
  stat_summary(geom = "line", fun.y = mean,
               aes(group = 1, linetype = "mean"), size = 1.5) +
  facet_wrap(~feedback, labeller = label_both)

```

## Hypothesis testing

The hypotheses being tested here are:  

1. Participants benefit from advice 

    a. when feedback is available
    
    b. and when feedback is not available
  
2. Participants use feedback to determine the quality of advice, and thus put more weight on accurate advisors in the feedback condition compared to the no-feedback condition

3. Participants denied feedback use agreement as a proxy for accuracy, and thus put more weight on agreeing advisors in the no-feedback condition compared to the feedback condition

All hypotheses are tested using the trials on which only one advisor provided advice.

### Benefits of advice

Participants should have *lower error on their last decisions than on their first decisions*.

```{r h1, results='asis'}

tmp <- aggregate(cbind(responseError, responseErrorFinal) ~ pid, 
                 block2, 
                 mean, na.rm = T)

r <- md.ttest(tmp$responseErrorFinal, tmp$responseError, 
              c("*M*|last", "*M*|first"), paired = T)
cat(r)

```

Supporting this primary result, participants should also score *more points on their last answer than on their first answer*, although given that they are allowed to change their confidence, they may sometimes miss the correct answer because they choose a thinner marker.

```{r h1score, results='asis'}

tmp <- aggregate(cbind(responseScore, responseScoreFinal) ~ pid, 
                 block2, 
                 mean, na.rm = T)

r <- md.ttest(tmp$responseScoreFinal, tmp$responseScore, 
              c("*M*|last", "*M*|first"), paired = T)
cat(r)

```

The above results could be because participants increase their confidence (i.e. not moving the centre of their marker and therefore not changing the error estimate, but changing the points scored). This can be assessed by looking for differences in correctness.

```{r h1correctness, results = 'asis'}

tmp <- aggregate(cbind(responseCorrect, responseCorrectFinal) ~ pid, 
                 block2, 
                 mean, na.rm = T)

r <- md.ttest(tmp$responseCorrectFinal, 
              tmp$responseCorrect, 
              c("*M*|last", "*M*|first"),
              paired = T)

cat(r)

```

#### with feedback

The above effect should hold for participants in the feedback condition.

```{r h1a, results='asis'}

tmp <- aggregate(cbind(responseError, responseErrorFinal) ~ pid, 
                 AdvisedTrial[AdvisedTrial$feedback, ], 
                 mean, na.rm = T)

r <- md.ttest(tmp$responseErrorFinal, tmp$responseError, 
              c("*M*|last", "*M*|first"),
              paired = T)
cat(r)

```

And for score as the outcome:

```{r h1aScore, results='asis'}

tmp <- aggregate(cbind(responseScore, responseScoreFinal) ~ pid, 
                 AdvisedTrial[AdvisedTrial$feedback, ], 
                 mean, na.rm = T)

r <- md.ttest(tmp$responseScoreFinal, tmp$responseScore, 
              c("*M*|last", "*M*|first"),
              paired = T)
cat(r)

```

#### without feedback

The effect should also hold for participants in the no-feedback condition.

```{r h1b, results='asis'}

tmp <- aggregate(cbind(responseError, responseErrorFinal) ~ pid, 
                 AdvisedTrial[!AdvisedTrial$feedback, ], 
                 mean, na.rm = T)

r <- md.ttest(tmp$responseErrorFinal, tmp$responseError, 
              c("*M*|last", "*M*|first"),
              paired = T)
cat(r)

```

And for score as the outcome:

```{r h1bScore, results='asis'}

tmp <- aggregate(cbind(responseScore, responseScoreFinal) ~ pid, 
                 AdvisedTrial[!AdvisedTrial$feedback, ], 
                 mean, na.rm = T)

r <- md.ttest(tmp$responseScoreFinal, tmp$responseScore, 
              c("*M*|last", "*M*|first"), 
              paired = T)
cat(r)

```

### Weighting of accuracy by feedback

Participants in the feedback condition should have *higher weight on advice for the accurate advisor* than participants in the no-feedback condition. 

```{r h2, results='asis'}

tmp <- aggregate(Accurate.woa ~ pid + feedback, 
                 AdvisedTrial, mean, na.rm = T)

r <- md.ttest(tmp$Accurate.woa[tmp$feedback], 
                tmp$Accurate.woa[!tmp$feedback],
                labels = c("*M*|feedback", "*M*|¬feedback"))
cat(r)

```

### Weighting of agreement by feedback

The opposite pattern is expected for the agreeing advisor: feedback should result in *lower weight on advice*. 

```{r h3, results='asis'}

tmp <- aggregate(Agreeing.woa ~ pid + feedback, 
                 AdvisedTrial, mean, na.rm = T)

r <- md.ttest(tmp$Agreeing.woa[tmp$feedback], 
                tmp$Agreeing.woa[!tmp$feedback],
                labels = c("*M*|feedback", "*M*|¬feedback"))
cat(r)

```

### General preference

We can also ask which advisor was preferred (if any) by participants in each condition:

```{r advisorPreference, results='asis'}

tmp <- aggregate(advisor0woa ~ 
                   pid + feedback + advisor0idDescription, 
                 block2, mean)

cat("Feedback: ")
cat(md.ttest(tmp$advisor0woa[tmp$feedback & 
                                   tmp$advisor0idDescription == "Agreeing"],
             tmp$advisor0woa[tmp$feedback & 
                                   tmp$advisor0idDescription != "Agreeing"],
             labels = c("*M*|agr", "*M*|acc"),
             paired = T))

cat("\n<br/>\nNo feedback: ")
cat(md.ttest(tmp$advisor0woa[!tmp$feedback & 
                                   tmp$advisor0idDescription == "Agreeing"],
             tmp$advisor0woa[!tmp$feedback & 
                                   tmp$advisor0idDescription != "Agreeing"],
             labels = c("*M*|agr", "*M*|acc"),
             paired = T))

```

### Bayesian ANOVA approach

The questions above can perhaps be more suitably answered using a heirachical Bayesian ANOVA analogue:

```{r bayesAnova}

# calculate woa for the advisor

tmp <- aggregate(advisor0woa ~ 
                   pid + feedback + advisor0idDescription + firstAdvisor,
                 block2, mean, na.rm = T)
tmp$feedback <- factor(tmp$feedback)

r <- BANOVA.Normal(l1_formula = advisor0woa ~ advisor0idDescription,
                   l2_formula = ~ feedback + firstAdvisor, 
                   data = tmp,
                   id = tmp$pid)

r

```

#### EZANOVA

Because I don't quite trust my usage of the BANOVA package above, we're also looking at this using EZ ANOVA:

```{r ezAnova}

library(ez)
r <- ezANOVA(tmp, advisor0woa, pid, 
             within = advisor0idDescription,
             between = list(feedback, firstAdvisor),
             detailed = T,
             return_aov = T,
             type = 2)

r

```

##### Offbrand trials

We suspect clearer results will be obtained using offbrand trials only, but due to there being substantially fewer offbrand than onbrand trials the data are likely to be noisier. 

```{r ezAnoveOffbrand}

tmp <- aggregate(advisor0woa ~ 
                   pid + feedback + advisor0idDescription + firstAdvisor,
                 offBrand, mean, na.rm = T)
tmp$feedback <- factor(tmp$feedback)

# remove incomplete cases
for (p in unique(tmp$pid)) {
  if (nrow(tmp[tmp$pid == p, ]) != 2) {
    print(paste("Dropping incomplete case pid =", p))
    tmp <- tmp[tmp$pid != p, ]
  }
}

# refactor pid
tmp$pid <- factor(tmp$pid)

r <- ezANOVA(tmp, advisor0woa, pid, 
             within = advisor0idDescription,
             between = list(feedback, firstAdvisor),
             detailed = T,
             return_aov = T,
             type = 2)

r

```

### Regressions

We suspect participants responses may be driven by properties of the advice only, while the properties of the advisor are ignored (i.e. participants behave as if all trials are discrete). If this is the case, there will be no differences between regression lines for advisors when regressing predictors vs WoA. 

#### Distance x WoA

Previous research indicates a u-shaped curve for distance x woa - first we check if this is appropriate.

```{r distanceWoAShape}

anova(lm(advisor0woa ~ advisor0distance, block2, na.action = na.exclude),
      lm(advisor0woa ~ poly(advisor0distance, 2), block2, na.action = na.exclude))

```

```{r distanceWoA}

theme_update(legend.position = 'bottom')

ggplot(block2, aes(x = advisor0distance, y = advisor0woa)) +
  geom_point(aes(shape = advisor0idDescription), alpha = 0.25, size = 3) +
  geom_smooth(aes(group = pid, linetype = advisor0idDescription),
              colour = "grey", se = F, method = 'lm', formula = y ~ poly(x, 2), 
              alpha = 0.05) +
  geom_smooth(aes(linetype = advisor0idDescription, 
                  fill = advisor0idDescription), 
              method = 'lm', formula = y ~ poly(x, 2)) +
  facet_wrap(~ feedback, labeller = label_both)

```

#### Advice error x WoA

```{r adviceErrorWoA}

ggplot(block2, aes(x = advisor0error, y = advisor0woa)) +
  geom_point(aes(shape = advisor0idDescription), alpha = 0.25, size = 3) +
  geom_smooth(aes(group = pid, linetype = advisor0idDescription),
              colour = "grey", se = F, method = 'lm', alpha = 0.05) +
  geom_smooth(aes(linetype = advisor0idDescription, 
                  fill = advisor0idDescription), 
              method = 'lm') +
  facet_wrap(~ feedback, labeller = label_both)

```

#### Initial decision error x WoA

```{r responseErrorWoA}

ggplot(block2, aes(x = responseError, y = advisor0woa)) +
  geom_point(aes(shape = advisor0idDescription), alpha = 0.25, size = 3) +
  geom_smooth(aes(group = pid, linetype = advisor0idDescription),
              colour = "grey", se = F, method = 'lm', alpha = 0.05) +
  geom_smooth(aes(linetype = advisor0idDescription, 
                  fill = advisor0idDescription), 
              method = 'lm') +
  facet_wrap(~ feedback, labeller = label_both)

```

#### Marker width x WoA

```{r confidenceWoA}

ggplot(block2, aes(x = responseMarkerWidth, y = advisor0woa)) +
  geom_point(aes(shape = advisor0idDescription), alpha = 0.25, size = 3) +
  geom_smooth(aes(group = pid, linetype = advisor0idDescription),
              colour = "grey", se = F, method = 'lm', alpha = 0.05) +
  geom_smooth(aes(linetype = advisor0idDescription, 
                  fill = advisor0idDescription), 
              method = 'lm') +
  scale_x_continuous(breaks = c(1, 3, 9)) +
  facet_wrap(~ feedback, labeller = label_both) 

```

## Questionnaires

### Advisors

First we want to take a look at how the advisor questionnaire data break down.

```{r advisorsQ}

debrief.advisors$uid <- paste0(debrief.advisors$pid, "_", 
                               debrief.advisors$advisorId)
advisors$uid <- paste0(advisors$pid, "_", advisors$id)

tmp <- left_join(debrief.advisors, advisors[, 6:ncol(advisors)], by = "uid")
tmp <- gather(tmp, "Q", "A", knowledge:likability)

tmp <- tmp[tmp$pid %in% AdvisedTrial$pid, ]

ggplot(tmp, aes(x = Q, y = A, colour = feedback)) + 
  geom_boxplot(fill = NA, outlier.color = NA) + 
  geom_point(position = position_jitterdodge(jitter.width = .1), alpha = .75) + 
  stat_summary(geom = "point", aes(group = feedback), fun.y = mean, 
               colour = "black", shape = 4, size = 5, 
               position = position_dodge(width = .75)) +
  facet_wrap(~idDescription)

```

Next we want to see whether participants who rate an advisor highly are also more influenced by that advisor's advice.

```{r advisorQcor}

tmp$woa <- sapply(seq(nrow(tmp)), function(i) 
  mean(pull(block2[block2$pid %in% tmp$pid[i], ],
            paste0(tmp$idDescription[i], ".woa")), na.rm = T))

ggplot(tmp, aes(x = A, y = woa, linetype = feedback, 
                fill = feedback, colour = feedback)) +
  geom_hline(yintercept = 0, linetype = "dashed", size = .5) +
  geom_hline(yintercept = 1, linetype = "dashed", size = .5) +
  geom_vline(xintercept = 100, linetype = "dashed", size = .5) +
  geom_vline(xintercept = 0, linetype = "dashed", size = .5) +
  geom_smooth(method = 'lm', alpha = .5) +
  geom_point(alpha = .5, size = 3) + 
  facet_grid(idDescription ~ Q) +
  theme(legend.position = 'bottom')

```

## Update modelling

We can model some of the changes investigated above as a function of the properties of each of the advisors on the trial. These models can be thought of as constituting more specifc hypotheses about the mechanics underlying the updating of advice quality on the basis of experience.

As a sanity check, we can run the established models of advisor updating as a consequence of feedback in the feedback condition. In this model, advisors' advice is adjusted as a consequence of the feedback received by an amount equal to a free learning rate parameter:
$$\omega_a^{t+1} = \omega_a^t + \lambda f(e_{a}^t, v^t)$$
where $\omega_a^t$ is the credibility of advisor $a$ on trial $t$, $\lambda$ is a learning rate parameter, $f(x,y)$ is a fitness function for an estimate $x$ and target value $y$, $e_a^t$ is the estimate provided by advisor $a$ on trial $t$, and $v^t$ the target value on trial $t$.

In the simplest case, $f(x, y)$ takes values of -1 or +1 for incorrect and correct estimates, respectively, while in other cases it may be continuous, e.g. the reciprocal of the error.

## Credits 

### Acknowledgements

Thanks as always to Nick Yeung and the other folks at the [ACC Lab](https://www.psy.ox.ac.uk/research/attention-cognitive-control-lab).

### R Packages

```{r results = 'asis'}
# list packages
packageNames <- (.packages())
# don't include very core package
packageNames <- packageNames[!(packageNames %in% 
                                 rownames(installed.packages(
                                   priority = "base")))]
# but do include the base package
packageNames <- c("base", packageNames)
out <- NULL
for (p in packageNames) {
  out <- rbind(out, data.frame('Package' = p, 
                               'Citations' = paste(format(citation(p), 
                                                          style = 'textVersion'), 
                                                   collapse = '<br/><br/>')))
}

kable(out)
```

### Funding

Matt Jaquiery is funded by a studentship from the [Medical Research Council](https://mrc.ukri.org/) (reference 1943590) and the University of Oxford [Department of Experimental Psychology](https://www.psy.ox.ac.uk/) (reference 17/18_MSD_661552).

### Technical details  

```{r results = 'hold'}
cat(paste('Time stamp:', Sys.time(), '\n\n'))
cat('Runtime \n')
proc.time()
cat('\n')
sessionInfo()
```